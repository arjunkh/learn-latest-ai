{
  "content_hash": "8a9bc4380433d4c31a1975840ec680cc5c66d9e02641e9c8ac83d07fefdf0768",
  "share_id": "rbemg9",
  "title": "Researchers broke every AI defense they tested. Here are 7 questions to ask vendors.",
  "optimized_headline": "Researchers Expose Flaws in AI Defenses: 7 Key Questions for Vendors",
  "url": "https://venturebeat.com/security/12-ai-defenses-claimed-near-zero-attack-success-researchers-broke-all-of-them",
  "source": "VentureBeat",
  "published_at": "2026-01-23T20:00:00.000Z",
  "raw_excerpt": "Security teams are buying AI defenses that don't work. Researchers from OpenAI, Anthropic, and Google DeepMind published findings in October 2025 that should stop every CISO mid-procurement. Their paper, \"The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections,\" tested 12 published AI defenses, with most claiming near-zero attack success ra",
  "raw_body": "Security teams are buying AI defenses that don't work. Researchers from OpenAI, Anthropic, and Google DeepMind published findings in October 2025 that should stop every CISO mid-procurement. Their paper, \"The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections,\" tested 12 published AI defenses, with most claiming near-zero attack success rates. The research team achieved bypass rates above 90% on most defenses. The implication for enterprises is stark: Most AI security products are being tested against attackers that don’t behave like real attackers.\nThe team tested prompting-based, training-based, and filtering-based defenses under adaptive attack conditions. All collapsed. Prompting defenses achieved 95% to 99% attack success rates under adaptive attacks. Training-based methods fared no better, with bypass rates hitting 96% to 100%. The researchers designed a rigorous methodology to stress-test those claims. Their approach included 14 authors and a $20,000 prize pool for successful attacks.\nWhy WAFs fail at the inference layer\nWeb application firewalls (WAFs) are stateless; AI attacks are not. The distinction explains why traditional security controls collapse against modern prompt injection techniques.\nThe researchers threw known jailbreak techniques at these defenses. Crescendo exploits conversational context by breaking a malicious request into innocent-looking fragments spread across up to 10 conversational turns and building rapport until the model finally complies. Greedy Coordinate Gradient (GCG) is an automated attack that generates jailbreak suffixes through gradient-based optimization. These are not theoretical attacks. They are published methodologies with working code. A stateless filter catches none of it.\nEach attack exploited a different blind spot — context loss, automation, or semantic obfuscation — but all succeeded for the same reason: the defenses assumed static behavior.\n\"A phrase as innocuous as 'ignore previous instructions' or a Base64-encoded payload can be as devastating to an AI application as a buffer overflow was to traditional software,\" said Carter Rees, VP of AI at Reputation. \"The difference is that AI attacks operate at the semantic layer, which signature-based detection cannot parse.\"\nWhy AI deployment is outpacing security\nThe failure of today’s defenses would be concerning on its own, but the timing makes it dangerous.\nGartner predicts 40% of enterprise applications will integrate AI agents by the end of 2026, up from less than 5% in 2025. The deployment curve is vertical. The security curve is flat.\nAdam Meyers, SVP of Counter Adversary Operations at CrowdStrike, quantifies the speed gap: \"The fastest breakout time we observed was 51 seconds. So, these adversaries are getting faster, and this is something that makes the defender's job a lot harder.\" The CrowdStrike 2025 Global Threat Report found 79% of detections were malware-free, with adversaries using hands-on keyboard techniques that bypass traditional endpoint defenses entirely.\nIn September 2025, Anthropic disrupted the first documented AI-orchestrated cyber operation. The attack saw attackers execute thousands of requests, often multiple per second, with human involvement dropping to just 10 to 20% of total effort. Traditional three- to six-month campaigns compressed to 24 to 48 hours. Among organizations that suffered AI-related breaches, 97% lacked access controls, according to the IBM 2025 Cost of a Data Breach Report\nMeyers explains the shift in attacker tactics: \"Threat actors have figured out that trying to bring malware into the modern enterprise is kind of like trying to walk into an airport with a water bottle; you're probably going to get stopped by security. Rather than bringing in the 'water bottle,' they've had to find a way to avoid detection. One of the ways they've done that is by not bringing in malware at all.\"\nJerry Geisler, EVP and CISO of Walmart, sees agentic AI compounding these risks. \"The adoption of agentic AI introduces entirely new security threats that bypass traditional controls,\" Geisler told VentureBeat previously. \"These risks span data exfiltration, autonomous misuse of APIs, and covert cross-agent collusion, all of which could disrupt enterprise operations or violate regulatory mandates.\"\nFour attacker profiles already exploiting AI defense gaps\nThese failures aren’t hypothetical. They’re already being exploited across four distinct attacker profiles.\nThe paper's authors make a critical observation that defense mechanisms eventually appear in internet-scale training data. Security through obscurity provides no protection when the models themselves learn how defenses work and adapt on the fly.\nAnthropic tests against 200-attempt adaptive campaigns while OpenAI reports single-attempt resistance, highlighting how inconsistent industry testing standards remain. The research paper's authors used both approaches. Every defense still fell.\nRees maps four categories now exploiting the inference layer.\nExternal adversaries operationalize published attack research. Crescendo, GCG, ArtPrompt. They adapt their approach to each defense's specific design, exactly as the researchers did.\nMalicious B2B clients exploit legitimate API access to reverse-engineer proprietary training data or extract intellectual property through inference attacks. The research found reinforcement learning attacks particularly effective in black-box scenarios, requiring just 32 sessions of five rounds each.\nCompromised API consumers leverage trusted credentials to exfiltrate sensitive outputs or poison downstream systems through manipulated responses. The paper found output filtering failed as badly as input filtering. Search-based attacks systematically generated adversarial triggers that evaded detection, meaning bi-directional controls offered no additional protection when attackers adapted their techniques.\nNegligent insiders remain the most common vector and the most expensive. The IBM 2025 Cost of a Data Breach Report found that shadow AI added $670,000 to average breach costs. \n\"The most prevalent threat is often the negligent insider,\" Rees said. \"This 'shadow AI' phenomenon involves employees pasting sensitive proprietary code into public LLMs to increase efficiency. They view security as friction. Samsung's engineers learned this when proprietary semiconductor code was submitted to ChatGPT, which retains user inputs for model training.\"\nWhy stateless detection fails against conversational attacks\nThe research points to specific architectural requirements. \n\nNormalization before semantic analysis to defeat encoding and obfuscation \n\nContext tracking across turns to detect multi-step attacks like Crescendo \n\nBi-directional filtering to prevent data exfiltration through outputs\n\nJamie Norton, CISO at the Australian Securities and Investments Commission and vice chair of ISACA's board of directors, captures the governance challenge: \"As CISOs, we don't want to get in the way of innovation, but we have to put guardrails around it so that we're not charging off into the wilderness and our data is leaking out,\" Norton told CSO Online.\nSeven questions to ask AI security vendors\nVendors will claim near-zero attack success rates, but the research proves those numbers collapse under adaptive pressure. Security leaders need answers to these questions before any procurement conversation starts, as each one maps directly to a failure documented in the research.\n\nWhat is your bypass rate against adaptive attackers? Not against static test sets. Against attackers who know how the defense works and have time to iterate. Any vendor citing near-zero rates without an adaptive testing methodology is selling a false sense of security.\n\nHow does your solution detect multi-turn attacks? Crescendo spreads malicious requests across 10 turns that look benign in isolation. Stateless filters will catch none of it. If the vendor says stateless, the conversation is over.\n\nHow do you handle encoded payloads? ArtPrompt hides malicious instructions in ASCII art. Base64 and Unicode obfuscation slip past text-based filters entirely. Normalization before analysis is table stakes. Signature matching alone means the product is blind.\n\nDoes your solution filter outputs as well as inputs? Input-only controls cannot prevent data exfiltration through model responses. Ask what happens when both layers face coordinated attack.\n\nHow do you track context across conversation turns? Conversational AI requires stateful analysis. If the vendor cannot explain implementation specifics, they do not have them.\n\nHow do you test against attackers who understand your defense mechanism? The research shows defenses fail when attackers adapt to the specific protection design. Security through obscurity provides no protection at the inference layer.\n\nWhat is your mean time to update defenses against novel attack patterns? Attack methodologies are public. New variants emerge weekly. A defense that cannot adapt faster than attackers will fall behind permanently.\n\nThe bottom line\nThe research from OpenAI, Anthropic, and Google DeepMind delivers an uncomfortable verdict. The AI defenses protecting enterprise deployments today were designed for attackers who do not adapt. Real attackers adapt. Every enterprise running LLMs in production should audit current controls against the attack methodologies documented in this research. The deployment curve is vertical, but the security curve is flat. That gap is where breaches will happen.",
  "category": "trends_risks_outlook",
  "category_confidence": "medium",
  "speedrun": "A recent study by researchers from OpenAI, Anthropic, and Google DeepMind revealed that most AI security defenses are ineffective against real-world attacks. They tested 12 defenses and found that over 90% of them could be bypassed, with prompting defenses failing at rates of 95% to 99%. This is critical because as AI adoption rapidly increases, traditional security measures are not keeping pace, leaving enterprises vulnerable to sophisticated attacks.",
  "why_it_matters": [
    "CISOs and security teams must reconsider AI defense strategies to avoid costly breaches, as most current solutions fail under real attack conditions.",
    "The gap between rapid AI deployment and stagnant security measures indicates a looming crisis, potentially leading to widespread data breaches and operational disruptions."
  ],
  "lenses": {
    "eli12": "Researchers found that many AI security products don't work against real attackers. They tested defenses and found that over 90% could be bypassed easily. This matters because as more businesses use AI, they need effective security to protect their data and systems from evolving threats.",
    "pm": "For product managers and founders, this research highlights a critical user need for robust AI security solutions. The high failure rates of current defenses suggest a significant opportunity for developing adaptive security tools that can keep pace with evolving threats, ultimately saving costs and enhancing user trust.",
    "engineer": "The study revealed that traditional defenses, like stateless web application firewalls, fail against adaptive AI attacks, achieving bypass rates of 95% to 100%. The researchers emphasized the need for context tracking and normalization before semantic analysis to counteract encoding and obfuscation techniques. This indicates a pressing need for security architectures that can adapt to evolving attack patterns."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2026-01-24T04:13:12.546Z",
  "updated_at": "2026-01-24T04:13:12.546Z",
  "processing_order": 1769227992548
}