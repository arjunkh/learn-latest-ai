{
  "content_hash": "c718be59eb961378ed86f8567fa4aa0a853b313e09197f0b90d2400732d37e33",
  "share_id": "tmbgsc",
  "title": "The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS",
  "optimized_headline": "Exploring Music Perception: Insights from the MUSE Benchmark on Audio LLMs",
  "url": "https://arxiv.org/abs/2510.19055",
  "source": "ArXiv AI",
  "published_at": "2025-10-23T04:00:00.000Z",
  "raw_excerpt": "arXiv:2510.19055v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) have demonstrated capabilities in audio understanding, but current evaluations may obscure fundamental weaknesses in relational reasoning. We introduce the Music Understanding and Structural Evaluation (MUSE) Benchmark, an open-source resource with 10 tasks designed to probe fundamental music perception skill",
  "raw_body": "arXiv:2510.19055v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) have demonstrated capabilities in audio understanding, but current evaluations may obscure fundamental weaknesses in relational reasoning. We introduce the Music Understanding and Structural Evaluation (MUSE) Benchmark, an open-source resource with 10 tasks designed to probe fundamental music perception skills. We evaluate four SOTA models (Gemini Pro and Flash, Qwen2.5-Omni, and Audio-Flamingo 3) against a large human baseline (N=200). Our results reveal a wide variance in SOTA capabilities and a persistent gap with human experts. While Gemini Pro succeeds on basic perception, Qwen and Audio Flamingo 3 perform at or near chance, exposing severe perceptual deficits. Furthermore, we find Chain-of-Thought (CoT) prompting provides inconsistent, often detrimental results. Our work provides a critical tool for evaluating invariant musical representations and driving development of more robust AI systems.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "The MUSE Benchmark has been introduced to evaluate how well Multimodal Large Language Models (MLLMs) understand music. It features 10 tasks and assesses four state-of-the-art models against a human baseline of 200 participants. The findings show significant gaps in performance, with some models like Qwen and Audio Flamingo 3 struggling to match even basic human perception. This matters now as it highlights the need for improved AI systems that can better grasp complex auditory relationships.",
  "why_it_matters": [
    "Musicians and music educators could benefit from insights into AI's current limitations in music perception, potentially guiding future tools and applications.",
    "The broader AI landscape could shift as developers focus on enhancing relational reasoning in audio models, addressing a critical area of weakness."
  ],
  "lenses": {
    "eli12": "The MUSE Benchmark tests how well AI can understand music, much like a quiz for students. It compares AI models to humans to see where they struggle. This matters because better AI could help in music education and create smarter music apps that understand our preferences.",
    "pm": "For product managers, the MUSE Benchmark highlights a gap in AI's ability to understand music, which could guide product development. Addressing these weaknesses could enhance user experience in music-related applications, making them more intuitive and engaging. This insight could also inform resource allocation for AI improvements.",
    "engineer": "The MUSE Benchmark evaluates four leading models, revealing that while Gemini Pro performs well, others like Qwen and Audio Flamingo 3 show significant perceptual deficits. The study indicates that Chain-of-Thought prompting often leads to inconsistent results. These findings underscore the need for more effective methods in training audio understanding models."
  },
  "hype_meter": 2,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-10-24T03:47:43.970Z",
  "updated_at": "2025-10-24T03:47:43.970Z",
  "processing_order": 1761277663973
}