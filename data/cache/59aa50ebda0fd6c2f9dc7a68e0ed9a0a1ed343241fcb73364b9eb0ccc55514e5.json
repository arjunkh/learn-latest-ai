{
  "content_hash": "59aa50ebda0fd6c2f9dc7a68e0ed9a0a1ed343241fcb73364b9eb0ccc55514e5",
  "share_id": "ira6lh",
  "title": "Inside Ring-1T: Ant engineers solve reinforcement learning bottlenecks at trillion scale",
  "optimized_headline": "Ant Engineers Tackle Trillion-Scale Reinforcement Learning Challenges in Ring-1T",
  "url": "https://venturebeat.com/ai/inside-ring-1t-ant-engineers-solve-reinforcement-learning-bottlenecks-at",
  "source": "VentureBeat",
  "published_at": "2025-10-24T04:00:00.000Z",
  "raw_excerpt": "China’s Ant Group, an affiliate of Alibaba, detailed technical information around its new model, Ring-1T, which the company said is “the first open-source reasoning model with one trillion total parameters.”\nRing-1T aims to compete with other reasoning models like GPT-5 and the o-series from OpenAI, as well as Google’s Gemini 2.5. With the new release of the latest model, Ant extends the geopoliti",
  "raw_body": "China’s Ant Group, an affiliate of Alibaba, detailed technical information around its new model, Ring-1T, which the company said is “the first open-source reasoning model with one trillion total parameters.”\nRing-1T aims to compete with other reasoning models like GPT-5 and the o-series from OpenAI, as well as Google’s Gemini 2.5. With the new release of the latest model, Ant extends the geopolitical debate over who will dominate the AI race: China or the US. \nAnt Group said Ring-1T is optimized for mathematical and logical problems, code generation and scientific problem-solving. \n“With approximately 50 billion activated parameters per token, Ring-1T achieves state-of-the-art performance across multiple challenging benchmarks — despite relying solely on natural language reasoning capabilities,” Ant said in a paper.\nRing-1T, which was first released on preview in September, adopts the same architecture as Ling 2.0 and trained on the Ling-1T-base model the company released earlier this month. Ant said this allows the model to support up to 128,000 tokens.\nTo train a model as large as Ring-1T, researchers had to develop new methods to scale reinforcement learning (RL).\nNew methods of training\n\nAnt Group developed three “interconnected innovations” to support the RL and training of Ring-1T, a challenge given the model's size and the typically large compute requirements it entails. These three are IcePop, C3PO++ and ASystem.\nIcePop removes noisy gradient updates to stabilize training without slowing inference. It helps eliminate catastrophic training-inference misalignment in RL. The researchers noted that when training models, particularly those using a mixture-of-experts (MoE) architecture like Ring-1T, there can often be a discrepancy in probability calculations. \n“This problem is particularly pronounced in the training of MoE models with RL due to the inherent usage of the dynamic routing mechanism. Additionally, in long CoT settings, these discrepancies can gradually accumulate across iterations and become further amplified,” the researchers said. \nIcePop “suppresses unstable training updates through double-sided masking calibration.”\nThe next new method the researchers had to develop is C3PO++, an improved version of the C3PO system that Ant previously established. The method manages how Ring-1T and other extra-large parameter models generate and process training examples, or what they call rollouts, so GPUs don’t sit idle. \nThe way it works would break work in rollouts into pieces to process in parallel. One group is the inference pool, which generates new data, and the other is the training pool, which collects results to update the model. C3PO++ creates a token budget to control how much data is processed, ensuring GPUs are used efficiently.\nThe last new method, ASystem, adopts a SingleController+SPMD (Single Program, Multiple Data) architecture to enable asynchronous operations.  \nBenchmark results\nAnt pointed Ring-1T to benchmarks measuring performance in mathematics, coding, logical reasoning and general tasks. They tested it against models such as DeepSeek-V3.1-Terminus-Thinking, Qwen-35B-A22B-Thinking-2507, Gemini 2.5 Pro and GPT-5 Thinking. \nIn benchmark testing, Ring-1T performed strongly, coming in second to OpenAI’s GPT-5 across most benchmarks. Ant said that Ring-1T showed the best performance among all the open-weight models it tested. \nThe model posted a 93.4% score on the AIME 25 leaderboard, second only to GPT-5. In coding, Ring-1T outperformed both DeepSeek and Qwen.\n“It indicates that our carefully synthesized dataset shapes Ring-1T’s robust performance on programming applications, which forms a strong foundation for future endeavors on agentic applications,” the company said. \nRing-1T shows how much Chinese companies are investing in models \nRing-1T is just the latest model from China aiming to dethrone GPT-5 and Gemini. \nChinese companies have been releasing impressive models at a quick pace since the surprise launch of DeepSeek in January. Ant's parent company, Alibaba, recently released Qwen3-Omni, a multimodal model that natively unifies text, image, audio and video. DeepSeek has also continued to improve its models and earlier this month, launched DeepSeek-OCR. This new model reimagines how models process information. \nWith Ring-1T and Ant’s development of new methods to train and scale extra-large models, the battle for AI dominance between the US and China continues to heat up.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "Ant Group has unveiled Ring-1T, the first open-source reasoning model with one trillion parameters, designed to tackle complex tasks like math and coding. It boasts 50 billion activated parameters per token, achieving impressive performance on various benchmarks, scoring 93.4% on the AIME 25 leaderboard, just behind OpenAI's GPT-5. This development highlights the accelerating competition in AI, particularly between China and the US, as both nations strive for technological supremacy.",
  "why_it_matters": [
    "Ant Group's advancements could enhance AI applications for developers and researchers, providing new tools for complex problem-solving.",
    "The release signals a significant shift in the AI landscape, with Chinese companies rapidly advancing, potentially reshaping global AI competition."
  ],
  "lenses": {
    "eli12": "Ant Group's Ring-1T is a massive AI model that can solve tough problems, like math and coding, using a trillion parameters. Think of it like a super-smart assistant that can handle a lot of information at once. This matters because it shows how quickly AI is evolving, giving people better tools to tackle complex tasks in their daily lives.",
    "pm": "For product managers, Ring-1T represents a new opportunity to leverage advanced AI for user applications, particularly in areas requiring logical reasoning and coding. The model's efficiency could reduce costs and improve performance, making it a valuable resource for developing innovative products. As competition heats up, staying informed about these advancements could help in strategic planning.",
    "engineer": "Technically, Ring-1T utilizes a mixture-of-experts architecture and innovative training methods like IcePop and C3PO++ to optimize performance and resource use. It achieves state-of-the-art results, notably scoring 93.4% on AIME 25, second only to GPT-5. These advancements in reinforcement learning techniques highlight the ongoing evolution in training large-scale models, pushing the boundaries of what AI can achieve."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-10-25T03:51:22.370Z",
  "updated_at": "2025-10-25T03:51:22.370Z",
  "processing_order": 1761364282372
}