{
  "content_hash": "79b3f8fb11d21348230ba92b3c3eca5170514e05812ae9eaaac9c4247abde377",
  "share_id": "hos2mk",
  "title": "How OpenAI is scaling the PostgreSQL database to 800 million users ",
  "optimized_headline": "OpenAI's Strategy to Scale PostgreSQL for 800 Million Users Revealed",
  "url": "https://venturebeat.com/data/how-openai-is-scaling-the-postgresql-database-to-800-million-users",
  "source": "VentureBeat",
  "published_at": "2026-01-23T20:00:00.000Z",
  "raw_excerpt": "While vector databases still have many valid use cases, organizations including OpenAI are leaning on PostgreSQL to get things done.\nIn a blog post on Thursday, OpenAI disclosed how it is using the open-source PostgreSQL database.\nOpenAI runs ChatGPT and its API platform for 800 million users on a single-primary PostgreSQL instance — not a distributed database, not a sharded cluster. One Azure Pos",
  "raw_body": "While vector databases still have many valid use cases, organizations including OpenAI are leaning on PostgreSQL to get things done.\nIn a blog post on Thursday, OpenAI disclosed how it is using the open-source PostgreSQL database.\nOpenAI runs ChatGPT and its API platform for 800 million users on a single-primary PostgreSQL instance — not a distributed database, not a sharded cluster. One Azure PostgreSQL Flexible Server handles all writes. Nearly 50 read replicas spread across multiple regions handle reads. The system processes millions of queries per second while maintaining low double-digit millisecond p99 latency and five-nines availability.\nThe setup challenges conventional scaling wisdom and offers enterprise architects insight into what actually works at massive scale.\nThe lesson here isn’t to copy OpenAI’s stack. It’s that architectural decisions should be driven by workload patterns and operational constraints — not by scale panic or fashionable infrastructure choices. OpenAI’s PostgreSQL setup shows how far proven systems can stretch when teams optimize deliberately instead of re-architecting prematurely.\n\"For years, PostgreSQL has been one of the most critical, under-the-hood data systems powering core products like ChatGPT and OpenAI’s API,\"  OpenAI engineer Bohan Zhang wrote in a technical disclosure. \"Over the past year, our PostgreSQL load has grown by more than 10x, and it continues to rise quickly.\"\nThe company achieved this scale through targeted optimizations, including connection pooling that cut connection time from 50 milliseconds to 5 milliseconds and cache locking to prevent 'thundering herd' problems where cache misses trigger database overload.\nWhy PostgreSQL matters for enterprises\nPostgreSQL handles operational data for ChatGPT and OpenAI's API platform. The workload is heavily read-oriented, which makes PostgreSQL a good fit. However, PostgreSQL's multiversion concurrency control (MVCC) creates challenges under heavy write loads. \nWhen updating data, PostgreSQL copies entire rows to create new versions, causing write amplification and forcing queries to scan through multiple versions to find current data.  \nRather than fighting this limitation, OpenAI built its strategy around it. At OpenAI’s scale, these tradeoffs aren’t theoretical — they determine which workloads stay on PostgreSQL and which ones must move elsewhere.\nHow OpenAI is optimizing PostgreSQL\nAt large scale, conventional database wisdom points to one of two paths: shard PostgreSQL across multiple primary instances so writes can be distributed, or migrate to a distributed SQL database like CockroachDB or YugabyteDB designed to handle massive scale from the start. Most organizations would have taken one of these paths years ago, well before reaching 800 million users.\nSharding or moving to a distributed SQL database eliminates the single-writer bottleneck. A distributed SQL database handles this coordination automatically, but both approaches introduce significant complexity: application code must route queries to the correct shard, distributed transactions become harder to manage and operational overhead increases substantially.\nInstead of sharding PostgreSQL, OpenAI established a hybrid strategy: no new tables in PostgreSQL. New workloads default to sharded systems like Azure Cosmos DB. Existing write-heavy workloads that can be horizontally partitioned get migrated out. Everything else stays in PostgreSQL with aggressive optimization.\nThis approach offers enterprises a practical alternative to wholesale re-architecture. Rather than spending years rewriting hundreds of endpoints, teams can identify specific bottlenecks and move only those workloads to purpose-built systems.  \nWhy this matters \nOpenAI's experience scaling PostgreSQL reveals several practices that enterprises can adopt regardless of their scale.\nBuild operational defenses at multiple layers. OpenAI's approach combines cache locking to prevent \"thundering herd\" problems, connection pooling (which dropped their connection time from 50ms to 5ms), and rate limiting at application, proxy and query levels. Workload isolation routes low-priority and high-priority traffic to separate instances, ensuring a poorly optimized new feature can't degrade core services. \nReview and monitor ORM-generated SQL in production. Object-Relational Mapping (ORM) frameworks like Django, SQLAlchemy, and Hibernate automatically generate database queries from application code, which is convenient for developers. However, OpenAI found one ORM-generated query joining 12 tables that caused multiple high-severity incidents when traffic spiked. The convenience of letting frameworks generate SQL creates hidden scaling risks that only surface under production load. Make reviewing these queries a standard practice.\nEnforce strict operational discipline. OpenAI permits only lightweight schema changes — anything triggering a full table rewrite is prohibited. Schema changes have a 5-second timeout. Long-running queries get automatically terminated to prevent blocking database maintenance operations. When backfilling data, they enforce rate limits so aggressive that operations can take over a week. \nRead-heavy workloads with burst writes can run on single-primary PostgreSQL longer than commonly assumed. The decision to shard should depend on workload patterns rather than user counts.\nThis approach is particularly relevant for AI applications, which often have heavily read-oriented workloads with unpredictable traffic spikes. These characteristics align with the pattern where single-primary PostgreSQL scales effectively.\nThe lesson is straightforward: identify actual bottlenecks, optimize proven infrastructure where possible, and migrate selectively when necessary. Wholesale re-architecture isn't always the answer to scaling challenges.",
  "category": "in_action_real_world",
  "category_confidence": "medium",
  "speedrun": "OpenAI has shared how it efficiently scales its PostgreSQL database to support 800 million users. Instead of using distributed systems, they rely on a single-primary PostgreSQL instance with nearly 50 read replicas, achieving millions of queries per second with low latency. This approach challenges traditional scaling methods, emphasizing optimization based on actual workloads rather than trendy solutions. Understanding this strategy is crucial as organizations seek to manage growing data demands effectively.",
  "why_it_matters": [
    "OpenAI's PostgreSQL setup directly impacts its ability to serve a vast user base efficiently, showcasing how targeted optimizations can enhance performance.",
    "This case highlights a broader trend where enterprises might reconsider conventional scaling methods, focusing on workload patterns rather than simply increasing infrastructure."
  ],
  "lenses": {
    "eli12": "OpenAI is using PostgreSQL to handle a huge number of users without spreading its system too thin. They manage tons of requests efficiently by using smart strategies like connection pooling. Think of it as a restaurant optimizing service without building a bigger kitchen. This matters because it shows that proven systems can still perform well, even at massive scales.",
    "pm": "For product managers, OpenAI's approach illustrates the importance of understanding user needs and operational limits. Instead of jumping to complex solutions, they focused on optimizing existing systems. This could lead to cost savings and improved efficiency. A practical takeaway is to assess specific bottlenecks and only shift workloads when necessary.",
    "engineer": "Technically, OpenAI's PostgreSQL setup processes millions of queries per second while maintaining a p99 latency in the low double digits and five-nines availability. They achieved a tenfold increase in load over the past year through optimizations like connection pooling, reducing connection times from 50 to 5 milliseconds. This approach, emphasizing workload patterns, challenges the common belief that sharding is always necessary at scale."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2026-01-24T04:13:03.656Z",
  "updated_at": "2026-01-24T04:13:03.656Z",
  "processing_order": 1769227983657
}