{
  "content_hash": "08bb0f509658a11f0c31c08973af3251d17e933598bf29f0717b71b6b2009ced",
  "share_id": "deabe4",
  "title": "DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey",
  "optimized_headline": "DeepSurvey-Bench: How Auto-Generated Surveys Measure Academic Impact Effectively",
  "url": "https://arxiv.org/abs/2601.15307",
  "source": "ArXiv AI",
  "published_at": "2026-01-23T05:00:00.000Z",
  "raw_excerpt": "arXiv:2601.15307v1 Announce Type: new \nAbstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys ",
  "raw_body": "arXiv:2601.15307v1 Announce Type: new \nAbstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep \"academic value\", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "The introduction of DeepSurvey-Bench aims to improve how we evaluate automatically generated scientific surveys. Existing benchmarks often rely on flawed criteria like citation counts and focus only on surface quality. DeepSurvey-Bench introduces a more comprehensive approach, assessing informational value, scholarly communication, and research guidance. This matters now as it could enhance the quality and reliability of scientific surveys generated by AI, impacting research quality overall.",
  "why_it_matters": [
    "Researchers and institutions could benefit from more reliable evaluations of AI-generated surveys, improving academic quality in their work.",
    "This shift could signal a broader change in how AI-generated content is assessed, emphasizing deeper academic value over superficial metrics."
  ],
  "lenses": {
    "eli12": "DeepSurvey-Bench is like a new report card for AI-generated surveys. Instead of just checking for neatness, it looks at how well the survey helps researchers understand complex topics. This is important for everyday people because better surveys can lead to clearer scientific insights that affect various aspects of life.",
    "pm": "For product managers and founders, DeepSurvey-Bench highlights a user need for more robust evaluation metrics in AI-generated content. By focusing on deeper academic value, it could lead to more effective tools for researchers, ultimately saving time and improving the quality of scientific outputs.",
    "engineer": "From a technical perspective, DeepSurvey-Bench addresses limitations in current benchmarks by focusing on three evaluation dimensions: informational value, scholarly communication, and research guidance. This approach could lead to more accurate assessments of AI-generated surveys, as extensive experiments show it aligns closely with human evaluations, indicating a significant advancement in the field."
  },
  "hype_meter": 2,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2026-01-24T04:11:35.999Z",
  "updated_at": "2026-01-24T04:11:35.999Z",
  "processing_order": 1769227896001
}