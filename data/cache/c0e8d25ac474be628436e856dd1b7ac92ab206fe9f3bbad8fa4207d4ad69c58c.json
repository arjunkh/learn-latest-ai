{
  "content_hash": "c0e8d25ac474be628436e856dd1b7ac92ab206fe9f3bbad8fa4207d4ad69c58c",
  "share_id": "wlszna",
  "title": "Why LinkedIn says prompting was a non-starter — and small models was the breakthrough",
  "optimized_headline": "LinkedIn's Surprising Shift: How Small Models Outperformed Prompting Techniques",
  "url": "https://venturebeat.com/infrastructure/why-linkedin-says-prompting-was-a-non-starter-and-small-models-was-the",
  "source": "VentureBeat",
  "published_at": "2026-01-21T23:30:00.000Z",
  "raw_excerpt": "LinkedIn is a leader in AI recommender systems, having developed them over the last 15-plus years. But getting to a next-gen recommendation stack for the job-seekers of tomorrow required a whole new technique. The company had to look beyond off-the-shelf models to achieve next-level accuracy, latency, and efficiency.\n“There was just no way we were gonna be able to do that through prompting,” Erran",
  "raw_body": "LinkedIn is a leader in AI recommender systems, having developed them over the last 15-plus years. But getting to a next-gen recommendation stack for the job-seekers of tomorrow required a whole new technique. The company had to look beyond off-the-shelf models to achieve next-level accuracy, latency, and efficiency.\n“There was just no way we were gonna be able to do that through prompting,” Erran Berger, VP of product engineering at LinkedIn, says in a new Beyond the Pilot podcast. “We didn't even try that for next-gen recommender systems because we realized it was a non-starter.”\nInstead, his team set to develop a highly detailed product policy document to fine-tune an initially massive 7-billion-parameter model; that was then further distilled into additional teacher and student models optimized to hundreds of millions of parameters. \nThe technique has created a repeatable cookbook now reused across LinkedIn’s AI products. \n“Adopting this eval process end to end will drive substantial quality improvement of the likes we probably haven't seen in years here at LinkedIn,” Berger says. \n\nWhy multi-teacher distillation was a ‘breakthrough’ for LinkedIn \nBerger and his team set out to build an LLM that could interpret individual job queries, candidate profiles and job descriptions in real time, and in a way that mirrored LinkedIn’s product policy as accurately as possible. \nWorking with the company's product management team, engineers eventually built out a 20-to-30-page document scoring job description and profile pairs “across many dimensions.” \n“We did many, many iterations on this,” Berger says. That product policy document was then paired with a “golden dataset” comprising thousands of pairs of queries and profiles; the team fed this into ChatGPT during data generation and experimentation, prompting the model over time to learn scoring pairs and eventually generate a much larger synthetic data set to train a 7-billion-parameter teacher model.\nHowever, Berger says, it's not enough to have an LLM running in production just on product policy. “At the end of the day, it's a recommender system, and we need to do some amount of click prediction and personalization.” \nSo, his team used that initial product policy-focused teacher model to develop a second teacher model oriented toward click prediction. Using the two, they further distilled a 1.7 billion parameter model for training purposes. That eventual student model was run through “many, many training runs,” and was optimized “at every point” to minimize quality loss, Berger says. \nThis multi-teacher distillation technique allowed the team to “achieve a lot of affinity” to the original product policy and “land” click prediction, he says. They were also able to “modularize and componentize” the training process for the student.\nConsider it in the context of a chat agent with two different teacher models: One is training the agent on accuracy in responses, the other on tone and how it should communicate. Those two things are very different, yet critical, objectives, Berger notes. \n“By now mixing them, you get better outcomes, but also iterate on them independently,” he says. “That was a breakthrough for us.” \nChanging how teams work together\nBerger says he can’t understate the importance of anchoring on a product policy and an iterative eval process. \nGetting a “really, really good product policy” requires translating product manager domain expertise into a unified document. Historically, Berger notes, the product management team was laser focused on strategy and user experience, leaving modeling iteration approaches to ML engineers. Now, though, the two teams work together to “dial in” and create an aligned teacher model. \n“How product managers work with machine learning engineers now is very different from anything we've done previously,” he says. “It’s now a blueprint for basically any AI products we do at LinkedIn.”\nWatch the full podcast to hear more about: \n\nHow LinkedIn optimized every step of the R&D process to support velocity, leading to real results with days or hours rather than weeks; \n\nWhy teams should develop pipelines for plugability and experimentation and try out different models to support flexibility; \n\nThe continued importance of traditional engineering debugging.\n\nYou can also listen and subscribe to Beyond the Pilot on Spotify, Apple or wherever you get your podcasts.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "LinkedIn has shifted its approach to AI recommender systems, moving away from traditional prompting methods to a new multi-teacher distillation technique. This involved developing a detailed product policy document and distilling a massive 7-billion-parameter model down to a more efficient 1.7 billion parameters. The new method enhances accuracy and personalization for job seekers. This transition is crucial for LinkedIn as it aims to improve user experience and operational efficiency in real-time job matching.",
  "why_it_matters": [
    "Job seekers could benefit from more accurate job recommendations tailored to their profiles, enhancing their chances of finding suitable employment.",
    "This innovation reflects a broader trend in AI development, where companies are increasingly focusing on customized solutions rather than relying on generic models."
  ],
  "lenses": {
    "eli12": "LinkedIn has revamped how it recommends jobs to users by moving from simple prompts to a detailed training process involving multiple models. Imagine a teacher guiding a student on both math and communication skills; this dual approach helps improve overall learning. This matters because better job recommendations could help many people land their dream jobs more effectively.",
    "pm": "For product managers, this shift highlights the importance of collaboration with engineering teams to create effective AI models. By developing a comprehensive product policy, they can ensure that user needs are met efficiently. This could lead to quicker iterations and better product outcomes, ultimately enhancing user satisfaction.",
    "engineer": "From a technical perspective, LinkedIn's use of multi-teacher distillation allows for more nuanced training of AI models. By combining a 7-billion-parameter teacher model focused on product policy with a click prediction model, they distilled down to a 1.7 billion parameter student model. This approach not only improves accuracy but also modularizes the training process, enhancing flexibility in future AI developments."
  },
  "hype_meter": 5,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2026-01-22T04:34:22.288Z",
  "updated_at": "2026-01-22T04:34:22.288Z",
  "processing_order": 1769056462288
}