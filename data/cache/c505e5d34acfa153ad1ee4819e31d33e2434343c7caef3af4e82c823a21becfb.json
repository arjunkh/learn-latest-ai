{
  "content_hash": "c505e5d34acfa153ad1ee4819e31d33e2434343c7caef3af4e82c823a21becfb",
  "share_id": "gatjeo",
  "title": "Goals and the Structure of Experience",
  "url": "https://arxiv.org/abs/2508.15013",
  "source": "ArXiv AI",
  "published_at": "2025-08-22T04:00:00.000Z",
  "raw_excerpt": "arXiv:2508.15013v1 Announce Type: new \nAbstract: Purposeful behavior is a hallmark of natural and artificial intelligence. Its acquisition is often believed to rely on world models, comprising both descriptive (what is) and prescriptive (what is desirable) aspects that identify and evaluate state of affairs in the world, respectively. Canonical computational accounts of purposeful behavior, such a",
  "raw_body": "arXiv:2508.15013v1 Announce Type: new \nAbstract: Purposeful behavior is a hallmark of natural and artificial intelligence. Its acquisition is often believed to rely on world models, comprising both descriptive (what is) and prescriptive (what is desirable) aspects that identify and evaluate state of affairs in the world, respectively. Canonical computational accounts of purposeful behavior, such as reinforcement learning, posit distinct components of a world model comprising a state representation (descriptive aspect) and a reward function (prescriptive aspect). However, an alternative possibility, which has not yet been computationally formulated, is that these two aspects instead co-emerge interdependently from an agent's goal. Here, we describe a computational framework of goal-directed state representation in cognitive agents, in which the descriptive and prescriptive aspects of a world model co-emerge from agent-environment interaction sequences, or experiences. Drawing on Buddhist epistemology, we introduce a construct of goal-directed, or telic, states, defined as classes of goal-equivalent experience distributions. Telic states provide a parsimonious account of goal-directed learning in terms of the statistical divergence between behavioral policies and desirable experience features. We review empirical and theoretical literature supporting this novel perspective and discuss its potential to provide a unified account of behavioral, phenomenological and neural dimensions of purposeful behaviors across diverse substrates.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "The article introduces a novel computational framework for understanding purposeful behavior in both natural and artificial intelligence. It posits that descriptive and prescriptive aspects of world models co-emerge from an agent's goals through interactions with the environment, challenging traditional reinforcement learning models. This approach has significant implications for enhancing goal-directed learning and understanding behavior across various cognitive systems.",
  "why_it_matters": [
    "This framework could revolutionize AI development by creating more adaptive and intelligent systems that learn from their experiences in a more human-like manner.",
    "By integrating insights from Buddhist epistemology, the research offers a unique perspective that bridges cognitive science and AI, potentially leading to new interdisciplinary applications."
  ],
  "lenses": {
    "eli12": "This article talks about how smart machines can learn better by understanding their goals. It suggests that their knowledge of what's happening and what they want can grow together. This is exciting because it might help machines become more like us in how they think and learn.",
    "pm": "Researchers and AI developers will use this framework to create smarter systems that learn from experiences. It addresses the limitation of traditional models that separate knowledge and goals, offering a competitive edge in adaptive learning. However, there are risks in implementation, as the new approach may require significant changes in existing AI architectures.",
    "engineer": "The proposed framework utilizes a novel approach where goal-directed state representations emerge from agent-environment interactions. This contrasts with traditional reinforcement learning that separates state and reward functions. While promising, the model may face limitations in scalability and complexity when applied to real-world scenarios, requiring further empirical validation."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v1.0"
  },
  "created_at": "2025-08-23T03:49:09.253Z",
  "updated_at": "2025-08-23T03:49:09.253Z",
  "processing_order": 1755920949254
}