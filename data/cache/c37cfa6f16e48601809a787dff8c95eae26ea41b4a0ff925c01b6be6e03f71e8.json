{
  "content_hash": "c37cfa6f16e48601809a787dff8c95eae26ea41b4a0ff925c01b6be6e03f71e8",
  "share_id": "mtma35",
  "title": "MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset",
  "optimized_headline": "Unlocking Emotions: Discover the MVRS Dataset for Virtual Reality Insights",
  "url": "https://arxiv.org/abs/2509.05330",
  "source": "ArXiv AI",
  "published_at": "2025-09-09T04:00:00.000Z",
  "raw_excerpt": "arXiv:2509.05330v1 Announce Type: new \nAbstract: Automatic emotion recognition has become increasingly important with the rise of AI, especially in fields like healthcare, education, and automotive systems. However, there is a lack of multimodal datasets, particularly involving body motion and physiological signals, which limits progress in the field. To address this, the MVRS dataset is introduce",
  "raw_body": "arXiv:2509.05330v1 Announce Type: new \nAbstract: Automatic emotion recognition has become increasingly important with the rise of AI, especially in fields like healthcare, education, and automotive systems. However, there is a lack of multimodal datasets, particularly involving body motion and physiological signals, which limits progress in the field. To address this, the MVRS dataset is introduced, featuring synchronized recordings from 13 participants aged 12 to 60 exposed to VR based emotional stimuli (relaxation, fear, stress, sadness, joy). Data were collected using eye tracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR signals (Arduino UNO), all timestamp aligned. Participants followed a unified protocol with consent and questionnaires. Features from each modality were extracted, fused using early and late fusion techniques, and evaluated with classifiers to confirm the datasets quality and emotion separability, making MVRS a valuable contribution to multimodal affective computing.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "The MVRS dataset has been introduced to enhance automatic emotion recognition, particularly in healthcare and education. It includes synchronized recordings from 13 participants aged 12 to 60 who experienced various emotional stimuli in a VR setting. Data was collected using eye tracking, body motion, and physiological signals, ensuring a comprehensive approach. This dataset addresses a significant gap in multimodal resources, which is crucial for advancing emotion recognition technologies.",
  "why_it_matters": [
    "This dataset could help researchers and developers create better AI systems that understand human emotions, improving user experiences in various applications.",
    "The introduction of MVRS signals a shift towards more integrated approaches in AI, emphasizing the importance of multimodal data for effective emotion recognition."
  ],
  "lenses": {
    "eli12": "The MVRS dataset helps computers recognize human emotions by combining different types of data, like body movement and physiological signals. Imagine a robot that can understand how you feel by watching your actions and measuring your heartbeat. This matters because it could lead to smarter technology that responds better to our emotional needs.",
    "pm": "For product managers and founders, the MVRS dataset highlights a growing user need for AI systems that can accurately interpret emotions. By leveraging this multimodal data, products could become more efficient at responding to users' emotional states, enhancing engagement and satisfaction. This could lead to better customer experiences across sectors like healthcare and education.",
    "engineer": "From a technical perspective, the MVRS dataset employs advanced methods like early and late fusion techniques to combine data from different sources, including eye tracking, Kinect body motion, and physiological signals. This approach allows for robust emotion classification, confirming the dataset's quality and effectiveness. Such multimodal integration is key for developing more nuanced AI systems capable of understanding complex human emotions."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-09-10T03:44:50.067Z",
  "updated_at": "2025-09-10T03:44:50.067Z",
  "processing_order": 1757475890070
}