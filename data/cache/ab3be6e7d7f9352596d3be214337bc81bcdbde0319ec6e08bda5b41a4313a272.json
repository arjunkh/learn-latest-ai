{
  "content_hash": "ab3be6e7d7f9352596d3be214337bc81bcdbde0319ec6e08bda5b41a4313a272",
  "share_id": "zsasp1",
  "title": "Zoom says it aced AI’s hardest exam. Critics say it copied off its neighbors.",
  "optimized_headline": "Zoom claims AI exam success, but critics allege it cheated.",
  "url": "https://venturebeat.com/ai/zoom-says-it-aced-ais-hardest-exam-critics-say-it-copied-off-its-neighbors",
  "source": "VentureBeat",
  "published_at": "2025-12-16T14:00:00.000Z",
  "raw_excerpt": "Zoom Video Communications, the company best known for keeping remote workers connected during the pandemic, announced last week that it had achieved the highest score ever recorded on one of artificial intelligence's most demanding tests — a claim that sent ripples of surprise, skepticism, and genuine curiosity through the technology industry.\nThe San Jose-based company said its AI system scored 4",
  "raw_body": "Zoom Video Communications, the company best known for keeping remote workers connected during the pandemic, announced last week that it had achieved the highest score ever recorded on one of artificial intelligence's most demanding tests — a claim that sent ripples of surprise, skepticism, and genuine curiosity through the technology industry.\nThe San Jose-based company said its AI system scored 48.1 percent on the Humanity's Last Exam, a benchmark designed by subject-matter experts worldwide to stump even the most advanced AI models. That result edges out Google's Gemini 3 Pro, which held the previous record at 45.8 percent.\n\"Zoom has achieved a new state-of-the-art result on the challenging Humanity's Last Exam full-set benchmark, scoring 48.1%, which represents a substantial 2.3% improvement over the previous SOTA result,\" wrote Xuedong Huang, Zoom's chief technology officer, in a blog post.\nThe announcement raises a provocative question that has consumed AI watchers for days: How did a video conferencing company — one with no public history of training large language models — suddenly vault past Google, OpenAI, and Anthropic on a benchmark built to measure the frontiers of machine intelligence?\nThe answer reveals as much about where AI is headed as it does about Zoom's own technical ambitions. And depending on whom you ask, it's either an ingenious demonstration of practical engineering or a hollow claim that appropriates credit for others' work.\nHow Zoom built an AI traffic controller instead of training its own model\nZoom did not train its own large language model. Instead, the company developed what it calls a \"federated AI approach\" — a system that routes queries to multiple existing models from OpenAI, Google, and Anthropic, then uses proprietary software to select, combine, and refine their outputs.\nAt the heart of this system sits what Zoom calls its \"Z-scorer,\" a mechanism that evaluates responses from different models and chooses the best one for any given task. The company pairs this with what it describes as an \"explore-verify-federate strategy,\" an agentic workflow that balances exploratory reasoning with verification across multiple AI systems.\n\"Our federated approach combines Zoom's own small language models with advanced open-source and closed-source models,\" Huang wrote. The framework \"orchestrates diverse models to generate, challenge, and refine reasoning through dialectical collaboration.\"\nIn simpler terms: Zoom built a sophisticated traffic controller for AI, not the AI itself.\nThis distinction matters enormously in an industry where bragging rights — and billions in valuation — often hinge on who can claim the most capable model. The major AI laboratories spend hundreds of millions of dollars training frontier systems on vast computing clusters. Zoom's achievement, by contrast, appears to rest on clever integration of those existing systems.\nWhy AI researchers are divided over what counts as real innovation\nThe response from the AI community was swift and sharply divided.\nMax Rumpf, an AI engineer who says he has trained state-of-the-art language models, posted a pointed critique on social media. \"Zoom strung together API calls to Gemini, GPT, Claude et al. and slightly improved on a benchmark that delivers no value for their customers,\" he wrote. \"They then claim SOTA.\"\nRumpf did not dismiss the technical approach itself. Using multiple models for different tasks, he noted, is \"actually quite smart and most applications should do this.\" He pointed to Sierra, an AI customer service company, as an example of this multi-model strategy executed effectively.\nHis objection was more specific: \"They did not train the model, but obfuscate this fact in the tweet. The injustice of taking credit for the work of others sits deeply with people.\"\nBut other observers saw the achievement differently. Hongcheng Zhu, a developer, offered a more measured assessment: \"To top an AI eval, you will most likely need model federation, like what Zoom did. An analogy is that every Kaggle competitor knows you have to ensemble models to win a contest.\"\nThe comparison to Kaggle — the competitive data science platform where combining multiple models is standard practice among winning teams — reframes Zoom's approach as industry best practice rather than sleight of hand. Academic research has long established that ensemble methods routinely outperform individual models.\nStill, the debate exposed a fault line in how the industry understands progress. Ryan Pream, founder of Exoria AI, was dismissive: \"Zoom are just creating a harness around another LLM and reporting that. It is just noise.\" Another commenter captured the sheer unexpectedness of the news: \"That the video conferencing app ZOOM developed a SOTA model that achieved 48% HLE was not on my bingo card.\"\nPerhaps the most pointed critique concerned priorities. Rumpf argued that Zoom could have directed its resources toward problems its customers actually face. \"Retrieval over call transcripts is not 'solved' by SOTA LLMs,\" he wrote. \"I figure Zoom's users would care about this much more than HLE.\"\nThe Microsoft veteran betting his reputation on a different kind of AI\nIf Zoom's benchmark result seemed to come from nowhere, its chief technology officer did not.\nXuedong Huang joined Zoom from Microsoft, where he spent decades building the company's AI capabilities. He founded Microsoft's speech technology group in 1993 and led teams that achieved what the company described as human parity in speech recognition, machine translation, natural language understanding, and computer vision.\nHuang holds a Ph.D. in electrical engineering from the University of Edinburgh. He is an elected member of the National Academy of Engineering and the American Academy of Arts and Sciences, as well as a fellow of both the IEEE and the ACM. His credentials place him among the most accomplished AI executives in the industry.\nHis presence at Zoom signals that the company's AI ambitions are serious, even if its methods differ from the research laboratories that dominate headlines. In his tweet celebrating the benchmark result, Huang framed the achievement as validation of Zoom's strategy: \"We have unlocked stronger capabilities in exploration, reasoning, and multi-model collaboration, surpassing the performance limits of any single model.\"\nThat final clause — \"surpassing the performance limits of any single model\" — may be the most significant. Huang is not claiming Zoom built a better model. He is claiming Zoom built a better system for using models.\nInside the test designed to stump the world's smartest machines\nThe benchmark at the center of this controversy, Humanity's Last Exam, was designed to be exceptionally difficult. Unlike earlier tests that AI systems learned to game through pattern matching, HLE presents problems that require genuine understanding, multi-step reasoning, and the synthesis of information across complex domains.\nThe exam draws on questions from experts around the world, spanning fields from advanced mathematics to philosophy to specialized scientific knowledge. A score of 48.1 percent might sound unimpressive to anyone accustomed to school grading curves, but in the context of HLE, it represents the current ceiling of machine performance.\n\"This benchmark was developed by subject-matter experts globally and has become a crucial metric for measuring AI's progress toward human-level performance on challenging intellectual tasks,\" Zoom’s announcement noted.\nThe company's improvement of 2.3 percentage points over Google's previous best may appear modest in isolation. But in competitive benchmarking, where gains often come in fractions of a percent, such a jump commands attention.\nWhat Zoom's approach reveals about the future of enterprise AI\nZoom's approach carries implications that extend well beyond benchmark leaderboards. The company is signaling a vision for enterprise AI that differs fundamentally from the model-centric strategies pursued by OpenAI, Anthropic, and Google.\nRather than betting everything on building the single most capable model, Zoom is positioning itself as an orchestration layer — a company that can integrate the best capabilities from multiple providers and deliver them through products that businesses already use every day.\nThis strategy hedges against a critical uncertainty in the AI market: no one knows which model will be best next month, let alone next year. By building infrastructure that can swap between providers, Zoom avoids vendor lock-in while theoretically offering customers the best available AI for any given task.\nThe announcement of OpenAI's GPT-5.2 the following day underscored this dynamic. OpenAI's own communications named Zoom as a partner that had evaluated the new model's performance \"across their AI workloads and saw measurable gains across the board.\" Zoom, in other words, is both a customer of the frontier labs and now a competitor on their benchmarks — using their own technology.\nThis arrangement may prove sustainable. The major model providers have every incentive to sell API access widely, even to companies that might aggregate their outputs. The more interesting question is whether Zoom's orchestration capabilities constitute genuine intellectual property or merely sophisticated prompt engineering that others could replicate.\nThe real test arrives when Zoom's 300 million users start asking questions\nZoom titled its announcement section on industry relations \"A Collaborative Future,\" and Huang struck notes of gratitude throughout. \"The future of AI is collaborative, not competitive,\" he wrote. \"By combining the best innovations from across the industry with our own research breakthroughs, we create solutions that are greater than the sum of their parts.\"\nThis framing positions Zoom as a beneficent integrator, bringing together the industry's best work for the benefit of enterprise customers. Critics see something else: a company claiming the prestige of an AI laboratory without doing the foundational research that earns it.\nThe debate will likely be settled not by leaderboards but by products. When AI Companion 3.0 reaches Zoom's hundreds of millions of users in the coming months, they will render their own verdict — not on benchmarks they have never heard of, but on whether the meeting summary actually captured what mattered, whether the action items made sense, whether the AI saved them time or wasted it.\nIn the end, Zoom's most provocative claim may not be that it topped a benchmark. It may be the implicit argument that in the age of AI, the best model is not the one you build — it's the one you know how to use.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "Zoom Video Communications announced it scored 48.1% on the challenging Humanity's Last Exam, surpassing Google's previous best of 45.8%. This achievement raised eyebrows as Zoom, primarily a video conferencing platform, didn't train its own AI model but instead used a federated approach, integrating outputs from existing models. The debate over this method highlights a significant shift in how AI capabilities can be harnessed, emphasizing the importance of effective system integration over singular model development.",
  "why_it_matters": [
    "Zoom's achievement impacts AI developers and researchers, illustrating that integration of existing models can yield high performance without extensive resources.",
    "This event signals a broader market shift toward orchestration strategies, where companies leverage multiple AI models rather than solely developing their own, potentially reshaping enterprise AI solutions."
  ],
  "lenses": {
    "eli12": "Zoom's recent claim of scoring high on an AI test has sparked debate. Instead of building its own AI, it cleverly combined existing models to achieve this result. Think of it like a chef using the best ingredients from various sources to create a delicious dish. This matters because it shows that sometimes, knowing how to mix and match can be just as valuable as creating something new from scratch.",
    "pm": "For product managers and founders, Zoom's approach illustrates a growing need to focus on user needs by integrating existing technologies rather than solely developing new models. This could lead to cost savings and improved efficiency in delivering AI solutions. The practical implication is that businesses might prioritize partnerships and integrations over building proprietary technology, which could reshape their product strategies.",
    "engineer": "From a technical perspective, Zoom used a federated AI approach, routing queries to various existing models and selecting the best outputs through its 'Z-scorer' mechanism. This method allowed Zoom to achieve a 2.3% improvement over Google's previous benchmark score. However, critics argue that this approach may lack the foundational innovation seen in traditional AI model development, raising questions about the authenticity of their claim."
  },
  "hype_meter": 2,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-12-17T04:08:21.676Z",
  "updated_at": "2025-12-17T04:08:21.676Z",
  "processing_order": 1765944501678
}