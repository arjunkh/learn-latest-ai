{
  "content_hash": "61dfdd27123a5caac3b1ed95e3ec0c25b7de6c93c86ffbcd55335cd6443f043e",
  "share_id": "nvr15s",
  "title": "Nvidia’s Vera Rubin is months away — Blackwell is getting faster right now",
  "optimized_headline": "Nvidia's Vera Rubin Launches Soon: How Blackwell's Speed Surprises Today",
  "url": "https://venturebeat.com/infrastructure/nvidias-vera-rubin-is-months-away-blackwell-is-getting-faster-right-now",
  "source": "VentureBeat",
  "published_at": "2026-01-09T05:00:00.000Z",
  "raw_excerpt": "The big news this week from Nvidia, splashed in headlines across all forms of media, was the company's announcement about its Vera Rubin GPU.\nThis week, Nvidia CEO Jensen Huang used his CES keynote to highlight performance metrics for the new chip. According to Huang, the Rubin GPU is capable of 50 PFLOPs of NVFP4 inference and 35 PFLOPs of NVFP4 training performance, representing 5x and 3.5x the ",
  "raw_body": "The big news this week from Nvidia, splashed in headlines across all forms of media, was the company's announcement about its Vera Rubin GPU.\nThis week, Nvidia CEO Jensen Huang used his CES keynote to highlight performance metrics for the new chip. According to Huang, the Rubin GPU is capable of 50 PFLOPs of NVFP4 inference and 35 PFLOPs of NVFP4 training performance, representing 5x and 3.5x the performance of Blackwell.\nBut it won't be available until the second half of 2026. So what should enterprises be doing now?\nBlackwell keeps on getting better\nThe current, shipping Nvidia GPU architecture is Blackwell, which was announced in 2024 as the successor to Hopper.  Alongside that release, Nvidia emphasized that that its product engineering path also included squeezing as much performance as possible out of the prior Grace Hopper architecture. \nIt's a direction that will hold true for Blackwell as well, with Vera Rubin coming later this year.\n\"We continue to optimize our inference and training stacks for the Blackwell architecture,\" Dave Salvator, director of accelerated computing products at Nvidia, told VentureBeat.\nIn the same week that Vera Rubin was being touted by Nvidia's CEO as its most powerful GPU ever, the company published new research showing improved Blackwell performance.\nHow Blackwell performance has improved inference by 2.8x \nNvidia has been able to increase Blackwell GPU performance by up to 2.8x per GPU in a period of just three short months.\nThe performance gains come from a series of innovations that have been added to the Nvidia TensorRT-LLM inference engine. These optimizations apply to existing hardware, allowing current Blackwell deployments to achieve higher throughput without hardware changes.\nThe performance gains are measured on DeepSeek-R1, a 671-billion parameter mixture-of-experts (MoE) model that activates 37 billion parameters per token. \nAmong the technical innovations that provide the performance boost:\n\nProgrammatic dependent launch (PDL): Expanded implementation reduces kernel launch latencies, increasing throughput.\n\nAll-to-all communication: New implementation of communication primitives eliminates an intermediate buffer, reducing memory overhead.\n\nMulti-token prediction (MTP): Generates multiple tokens per forward pass rather than one at a time, increasing throughput across various sequence lengths.\n\nNVFP4 format: A 4-bit floating point format with hardware acceleration in Blackwell that reduces memory bandwidth requirements while preserving model accuracy.\n\nThe optimizations reduce cost per million tokens and allow existing infrastructure to serve higher request volumes at lower latency. Cloud providers and enterprises can scale their AI services without immediate hardware upgrades.\nBlackwell has also made training performance gains \nBlackwell is also widely used as a foundational hardware component for training the largest of large language models.\nIn that respect, Nvidia has also reported significant gains for Blackwell when used for AI training. \nSince its initial launch, the GB200 NVL72 system delivered up to 1.4x higher training performance on the same hardware — a 40% boost achieved in just five months without any hardware upgrades.\nThe training boost came from a series of updates including:\n\nOptimized training recipes. Nvidia engineers developed sophisticated training recipes that effectively leverage NVFP4 precision. Initial Blackwell submissions used FP8 precision, but the transition to NVFP4-optimized recipes unlocked substantial additional performance from the existing silicon.\n\nAlgorithmic refinements. Continuous software stack enhancements and algorithmic improvements enabled the platform to extract more performance from the same hardware, demonstrating ongoing innovation beyond initial deployment.\n\nDouble-down on Blackwell or wait for Vera Rubin?\nSalvator noted that the high-end Blackwell Ultra is a market-leading platform purpose-built to run state-of-the-art AI models and applications. \nHe added that the Nvidia Rubin platform will extend the company's market leadership and enable the next generation of MoEs to power a new class of applications to take AI innovation even further.\nSalvator explained that the Vera Rubin is built to address the growing demand in compute created by the continuing growth in model size and reasoning token generation from leading models such as MoE.  \n \"Blackwell and Rubin can serve the same models, but the difference is the performance, efficiency and token cost,\" he said.\nAccording to Nvidia's early testing results, compared to Blackwell, Rubin can train large MoE models in a quarter the number of GPUs, inference token generation with 10X more throughput per watt, and inference at 1/10th the cost per token.\n\"Better token throughput performance and efficiency, means newer models can be built with more reasoning capability and faster agent-to-agent interaction, creating better intelligence at lower cost,\" Salvator said.\nWhat it all means for enterprise AI builders\nFor enterprises deploying AI infrastructure today, current investments in Blackwell remain sound despite Vera Rubin's arrival later this year.\nOrganizations with existing Blackwell deployments can immediately capture the 2.8x inference improvement and 1.4x training boost by updating to the latest TensorRT-LLM versions — delivering real cost savings without capital expenditure. For those planning new deployments in the first half of 2026, proceeding with Blackwell makes sense. Waiting six months means delaying AI initiatives and potentially falling behind competitors already deploying today.\nHowever, enterprises planning large-scale infrastructure buildouts for late 2026 and beyond should factor Vera Rubin into their roadmaps. The 10x improvement in throughput per watt and 1/10th cost per token represent transformational economics for AI operations at scale.\nThe smart approach is phased deployment: Leverage Blackwell for immediate needs while architecting systems that can incorporate Vera Rubin when available. Nvidia's continuous optimization model means this isn't a binary choice; enterprises can maximize value from current deployments without sacrificing long-term competitiveness.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "Nvidia recently unveiled its Vera Rubin GPU, boasting impressive performance metrics of 50 PFLOPs for inference and 35 PFLOPs for training, which are 5x and 3.5x better than the current Blackwell architecture. However, Vera Rubin won't be available until late 2026. Meanwhile, Nvidia has improved Blackwell's performance by up to 2.8x for inference and 1.4x for training, allowing companies to enhance their existing AI capabilities without new hardware. This ongoing optimization is crucial as AI demand continues to grow.",
  "why_it_matters": [
    "Businesses using Blackwell can immediately benefit from performance boosts, enhancing efficiency and reducing costs without new investments.",
    "The advancements signal a broader trend in AI infrastructure, emphasizing the importance of continuous optimization and performance gains in a competitive market."
  ],
  "lenses": {
    "eli12": "Nvidia's new Vera Rubin GPU promises much faster processing than its current Blackwell model, but it won't be ready until 2026. In the meantime, existing Blackwell users can upgrade their systems to see improvements right away, like speeding up AI tasks. This is important for everyday people because it means AI services will become faster and cheaper, leading to better applications and experiences.",
    "pm": "For product managers and founders, the enhancements in Blackwell mean immediate cost savings and efficiency gains for AI projects. Companies can leverage the current architecture to meet user needs without waiting for the new Vera Rubin GPU. This approach allows for faster deployment of AI strategies while planning for future upgrades, ensuring they remain competitive.",
    "engineer": "From a technical perspective, Nvidia has achieved significant performance improvements in Blackwell through updates to its TensorRT-LLM inference engine, resulting in a 2.8x boost in inference performance. Innovations like programmatic dependent launch and multi-token prediction have optimized existing hardware, allowing for better throughput. These enhancements highlight the potential for ongoing performance gains in AI systems without the need for new hardware investments."
  },
  "hype_meter": 4,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2026-01-10T04:10:28.997Z",
  "updated_at": "2026-01-10T04:10:28.997Z",
  "processing_order": 1768018229000
}