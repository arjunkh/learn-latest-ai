{
  "content_hash": "d295a5b1fba80ba16b9414bc97c8cd61bfd754f1d28f0711a31a80f5d38d5abd",
  "share_id": "tlamn9",
  "title": "Terminal-Bench 2.0 launches alongside Harbor, a new framework for testing agents in containers",
  "optimized_headline": "\"Terminal-Bench 2.0 Debuts with Harbor: A Game-Changer for Container Testing\"",
  "url": "https://venturebeat.com/ai/terminal-bench-2-0-launches-alongside-harbor-a-new-framework-for-testing",
  "source": "VentureBeat",
  "published_at": "2025-11-07T23:25:00.000Z",
  "raw_excerpt": "The developers of Terminal-Bench, a benchmark suite for evaluating the performance of autonomous AI agents on real-world terminal-based tasks, have released version 2.0 alongside Harbor, a new framework for testing, improving and optimizing AI agents in containerized environments. \nThe dual release aims to address long-standing pain points in testing and optimizing AI agents, particularly those bu",
  "raw_body": "The developers of Terminal-Bench, a benchmark suite for evaluating the performance of autonomous AI agents on real-world terminal-based tasks, have released version 2.0 alongside Harbor, a new framework for testing, improving and optimizing AI agents in containerized environments. \nThe dual release aims to address long-standing pain points in testing and optimizing AI agents, particularly those built to operate autonomously in realistic developer environments.\nWith a more difficult and rigorously verified task set, Terminal-Bench 2.0 replaces version 1.0 as the standard for assessing frontier model capabilities. \nHarbor, the accompanying runtime framework, enables developers and researchers to scale evaluations across thousands of cloud containers and integrates with both open-source and proprietary agents and training pipelines.\n“Harbor is the package we wish we had had while making Terminal-Bench,\" wrote co-creator Alex Shaw on X. \"It’s for agent, model, and benchmark developers and researchers who want to evaluate and improve agents and models.\"\nHigher Bar, Cleaner Data\nTerminal-Bench 1.0 saw rapid adoption after its release in May 2025, becoming a default benchmark for evaluating agent performance across the field of AI-powered agents operating in developer-style terminal environments. These agents interact with systems through the command line, mimicking how developers work behind the scenes of the graphical user interface.\nHowever, its broad scope came with inconsistencies. Several tasks were identified by the community as poorly specified or unstable due to external service changes.\nVersion 2.0 addresses those issues directly. The updated suite includes 89 tasks, each subjected to several hours of manual and LLM-assisted validation. The emphasis is on making tasks solvable, realistic, and clearly specified, raising the difficulty ceiling while improving reliability and reproducibility.\nA notable example is the download-youtube task, which was removed or refactored in 2.0 due to its dependence on unstable third-party APIs.\n“Astute Terminal-Bench fans may notice that SOTA performance is comparable to TB1.0 despite our claim that TB2.0 is harder,” Shaw noted on X. “We believe this is because task quality is substantially higher in the new benchmark.”\nHarbor: Unified Rollouts at Scale\nAlongside the benchmark update, the team launched Harbor, a new framework for running and evaluating agents in cloud-deployed containers. \nHarbor supports large-scale rollout infrastructure, with compatibility for major providers like Daytona and Modal.\nDesigned to generalize across agent architectures, Harbor supports:\n\nEvaluation of any container-installable agent\n\nScalable supervised fine-tuning (SFT) and reinforcement learning (RL) pipelines\n\nCustom benchmark creation and deployment\n\nFull integration with Terminal-Bench 2.\n\nHarbor was used internally to run tens of thousands of rollouts during the creation of the new benchmark. It is now publicly available via harborframework.com, with documentation for testing and submitting agents to the public leaderboard.\nEarly Results: GPT-5 Leads in Task Success\nInitial results from the Terminal-Bench 2.0 leaderboard show OpenAI's Codex CLI (command line interface), a GPT-5 powered variant, in the lead, with a 49.6% success rate — the highest among all agents tested so far. \nClose behind are other GPT-5 variants and Claude Sonnet 4.5-based agents.\nTop 5 Agent Results (Terminal-Bench 2.0):\n\nCodex CLI (GPT-5) — 49.6%\n\nCodex CLI (GPT-5-Codex) — 44.3%\n\nOpenHands (GPT-5) — 43.8%\n\nTerminus 2 (GPT-5-Codex) — 43.4%\n\nTerminus 2 (Claude Sonnet 4.5) — 42.8%\n\nThe close clustering among top models indicates active competition across platforms, with no single agent solving more than half the tasks.\nSubmission and Use\nTo test or submit an agent, users install Harbor and run the benchmark using simple CLI commands. Submissions to the leaderboard require five benchmark runs, and results can be emailed to the developers along with job directories for validation.\nharbor run -d terminal-bench@2.0 -m \"<model>\" -a \"<agent>\" --n-attempts 5 --jobs-dir <path/to/output>\nTerminal-Bench 2.0 is already being integrated into research workflows focused on agentic reasoning, code generation, and tool use. According to co-creator Mike Merrill, a postdoctoral researcher at Stanford, a detailed preprint is in progress covering the verification process and design methodology behind the benchmark.\nAiming for Standardization\nThe combined release of Terminal-Bench 2.0 and Harbor marks a step toward more consistent and scalable agent evaluation infrastructure. As LLM agents proliferate in developer and operational environments, the need for controlled, reproducible testing has grown.\nThese tools offer a potential foundation for a unified evaluation stack — supporting model improvement, environment simulation, and benchmark standardization across the AI ecosystem.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "Terminal-Bench 2.0 has launched alongside Harbor, a new framework for testing AI agents in containerized environments. This update introduces 89 rigorously validated tasks, improving reliability and raising the difficulty compared to version 1.0. Initial results show OpenAI's Codex CLI, powered by GPT-5, leading with a 49.6% success rate. This advancement matters now as it addresses inconsistencies in AI agent evaluations, paving the way for better performance in real-world applications.",
  "why_it_matters": [
    "Developers and researchers can now evaluate AI agents more effectively, leading to improved performance in real-world tasks.",
    "This release signals a shift toward standardized testing methods in AI, which could enhance the overall quality and reliability of AI systems."
  ],
  "lenses": {
    "eli12": "Terminal-Bench 2.0 is like a new set of rules for a game, making it harder but fairer for AI agents. It improves the way we test how well these agents can perform tasks, ensuring they work better in real-life situations. This matters for everyday people because it could lead to smarter AI tools that help us with various tasks more reliably.",
    "pm": "For product managers and founders, Terminal-Bench 2.0 and Harbor offer a clearer way to assess AI agent performance, which could lead to more effective product development. The improved testing framework helps identify user needs more accurately, potentially reducing costs and increasing efficiency. This means teams could iterate on their AI products faster and with more confidence.",
    "engineer": "From a technical perspective, Terminal-Bench 2.0 introduces 89 tasks that have undergone extensive validation, enhancing the robustness of performance assessments. Harbor allows for scalable deployment and evaluation of agents across cloud containers, supporting various architectures. This combination could facilitate more reliable benchmarking and optimization of AI models in real-world scenarios."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-11-08T03:50:22.646Z",
  "updated_at": "2025-11-08T03:50:22.646Z",
  "processing_order": 1762573822646
}