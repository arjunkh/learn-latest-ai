{
  "content_hash": "65f97244499e4ecb95ace6c60caedaa221ad4af8411b3b6b78d7d71f7ec16549",
  "share_id": "bth18v",
  "title": "Baseten takes on hyperscalers with new AI training platform that lets you own your model weights",
  "optimized_headline": "Baseten launches AI platform enabling ownership of model weights against hyperscalers",
  "url": "https://venturebeat.com/ai/baseten-takes-on-hyperscalers-with-new-ai-training-platform-that-lets-you",
  "source": "VentureBeat",
  "published_at": "2025-11-10T14:00:00.000Z",
  "raw_excerpt": "Baseten, the AI infrastructure company recently valued at $2.15 billion, is making its most significant product pivot yet: a full-scale push into model training that could reshape how enterprises wean themselves off dependence on OpenAI and other closed-source AI providers.\nThe San Francisco-based company announced Thursday the general availability of Baseten Training, an infrastructure platform d",
  "raw_body": "Baseten, the AI infrastructure company recently valued at $2.15 billion, is making its most significant product pivot yet: a full-scale push into model training that could reshape how enterprises wean themselves off dependence on OpenAI and other closed-source AI providers.\nThe San Francisco-based company announced Thursday the general availability of Baseten Training, an infrastructure platform designed to help companies fine-tune open-source AI models without the operational headaches of managing GPU clusters, multi-node orchestration, or cloud capacity planning. The move is a calculated expansion beyond Baseten's core inference business, driven by what CTO Amir Haghighat describes as relentless customer demand and a strategic imperative to capture the full lifecycle of AI deployment.\n\"We had a captive audience of customers who kept coming to us saying, 'Hey, I hate this problem,'\" Haghighat said in an interview. \"One of them told me, 'Look, I bought a bunch of H100s from a cloud provider. I have to SSH in on Friday, run my fine-tuning job, then check on Monday to see if it worked. Sometimes I realize it just hasn't been working all along.'\"\nThe launch comes at a critical inflection point in enterprise AI adoption. As open-source models from Meta, Alibaba, and others increasingly rival proprietary systems in performance, companies face mounting pressure to reduce their reliance on expensive API calls to services like OpenAI's GPT-5 or Anthropic's Claude. But the path from off-the-shelf open-source model to production-ready custom AI remains treacherous, requiring specialized expertise in machine learning operations, infrastructure management, and performance optimization.\nBaseten's answer: provide the infrastructure rails while letting companies retain full control over their training code, data, and model weights. It's a deliberately low-level approach born from hard-won lessons.\nHow a failed product taught Baseten what AI training infrastructure really needs\nThis isn't Baseten's first foray into training. The company's previous attempt, a product called Blueprints launched roughly two and a half years ago, failed spectacularly — a failure Haghighat now embraces as instructive.\n\"We had created the abstraction layer a little too high,\" he explained. \"We were trying to create a magical experience, where as a user, you come in and programmatically choose a base model, choose your data and some hyperparameters, and magically out comes a model.\"\nThe problem? Users didn't have the intuition to make the right choices about base models, data quality, or hyperparameters. When their models underperformed, they blamed the product. Baseten found itself in the consulting business rather than the infrastructure business, helping customers debug everything from dataset deduplication to model selection.\n\"We became consultants,\" Haghighat said. \"And that's not what we had set out to do.\"\nBaseten killed Blueprints and refocused entirely on inference, vowing to \"earn the right\" to expand again. That moment arrived earlier this year, driven by two market realities: the vast majority of Baseten's inference revenue comes from custom models that customers train elsewhere, and competing training platforms were using restrictive terms of service to lock customers into their inference products.\n\"Multiple companies who were building fine-tuning products had in their terms of service that you as a customer cannot take the weights of the fine-tuned model with you somewhere else,\" Haghighat said. \"I understand why from their perspective — I still don't think there is a big company to be made purely on just training or fine-tuning. The sticky part is in inference, the valuable part where value is unlocked is in inference, and ultimately the revenue is in inference.\"\nBaseten took the opposite approach: customers own their weights and can download them at will. The bet is that superior inference performance will keep them on the platform anyway.\nMulti-cloud GPU orchestration and sub-minute scheduling set Baseten apart from hyperscalers\nThe new Baseten Training product operates at what Haghighat calls \"the infrastructure layer\" — lower-level than the failed Blueprints experiment, but with opinionated tooling around reliability, observability, and integration with Baseten's inference stack.\nKey technical capabilities include multi-node training support across clusters of NVIDIA H100 or B200 GPUs, automated checkpointing to protect against node failures, sub-minute job scheduling, and integration with Baseten's proprietary Multi-Cloud Management (MCM) system. That last piece is critical: MCM allows Baseten to dynamically provision GPU capacity across multiple cloud providers and regions, passing cost savings to customers while avoiding the capacity constraints and multi-year contracts typical of hyperscaler deals.\n\"With hyperscalers, you don't get to say, 'Hey, give me three or four B200 nodes while my job is running, and then take it back from me and don't charge me for it,'\" Haghighat said. \"They say, 'No, you need to sign a three-year contract.' We don't do that.\"\nBaseten's approach mirrors broader trends in cloud infrastructure, where abstraction layers increasingly allow workloads to move fluidly across providers. When AWS experienced a major outage several weeks ago, Baseten's inference services remained operational by automatically routing traffic to other cloud providers — a capability now extended to training workloads.\nThe technical differentiation extends to Baseten's observability tooling, which provides per-GPU metrics for multi-node jobs, granular checkpoint tracking, and a refreshed UI that surfaces infrastructure-level events. The company also introduced an \"ML Cookbook\" of open-source training recipes for popular models like Gemma, GPT OSS, and Qwen, designed to help users reach \"training success\" faster.\nEarly adopters report 84% cost savings and 50% latency improvements with custom models\nTwo early customers illustrate the market Baseten is targeting: AI-native companies building specialized vertical solutions that require custom models.\nOxen AI, a platform focused on dataset management and model fine-tuning, exemplifies the partnership model Baseten envisions. CEO Greg Schoeninger articulated a common strategic calculus, telling VentureBeat: \"Whenever I've seen a platform try to do both hardware and software, they usually fail at one of them. That's why partnering with Baseten to handle infrastructure was the obvious choice.\"\nOxen built its customer experience entirely on top of Baseten's infrastructure, using the Baseten CLI to programmatically orchestrate training jobs. The system automatically provisions and deprovisions GPUs, fully concealing Baseten's interface behind Oxen's own. For one Oxen customer, AlliumAI — a startup bringing structure to messy retail data — the integration delivered 84% cost savings compared to previous approaches, reducing total inference costs from $46,800 to $7,530.\n\"Training custom LoRAs has always been one of the most effective ways to leverage open-source models, but it often came with infrastructure headaches,\" said Daniel Demillard, CEO of AlliumAI. \"With Oxen and Baseten, that complexity disappears. We can train and deploy models at massive scale without ever worrying about CUDA, which GPU to choose, or shutting down servers after training.\"\nParsed, another early customer, tackles a different pain point: helping enterprises reduce dependence on OpenAI by creating specialized models that outperform generalist LLMs on domain-specific tasks. The company works in mission-critical sectors like healthcare, finance, and legal services, where model performance and reliability aren't negotiable.\n\"Prior to switching to Baseten, we were seeing repetitive and degraded performance on our fine-tuned models due to bugs with our previous training provider,\" said Charles O'Neill, Parsed's co-founder and chief science officer. \"On top of that, we were struggling to easily download and checkpoint weights after training runs.\"\nWith Baseten, Parsed achieved 50% lower end-to-end latency for transcription use cases, spun up HIPAA-compliant EU deployments for testing within 48 hours, and kicked off more than 500 training jobs. The company also leveraged Baseten's modified vLLM inference framework and speculative decoding — a technique that generates draft tokens to accelerate language model output — to cut latency in half for custom models.\n\"Fast models matter,\" O'Neill said. \"But fast models that get better over time matter more. A model that's 2x faster but static loses to one that's slightly slower but improving 10% monthly. Baseten gives us both — the performance edge today and the infrastructure for continuous improvement.\"\nWhy training and inference are more interconnected than the industry realizes\nThe Parsed example illuminates a deeper strategic rationale for Baseten's training expansion: the boundary between training and inference is blurrier than conventional wisdom suggests.\nBaseten's model performance team uses the training platform extensively to create \"draft models\" for speculative decoding, a cutting-edge technique that can dramatically accelerate inference. The company recently announced it achieved 650+ tokens per second on OpenAI's GPT OSS 120B model — a 60% improvement over its launch performance — using EAGLE-3 speculative decoding, which requires training specialized small models to work alongside larger target models.\n\"Ultimately, inference and training plug in more ways than one might think,\" Haghighat said. \"When you do speculative decoding in inference, you need to train the draft model. Our model performance team is a big customer of the training product to train these EAGLE heads on a continuous basis.\"\nThis technical interdependence reinforces Baseten's thesis that owning both training and inference creates defensible value. The company can optimize the entire lifecycle: a model trained on Baseten can be deployed with a single click to inference endpoints pre-optimized for that architecture, with deployment-from-checkpoint support for chat completion and audio transcription workloads.\nThe approach contrasts sharply with vertically integrated competitors like Replicate or Modal, which also offer training and inference but with different architectural tradeoffs. Baseten's bet is on lower-level infrastructure flexibility and performance optimization, particularly for companies running custom models at scale.\nAs open-source AI models improve, enterprises see fine-tuning as the path away from OpenAI dependency\nUnderpinning Baseten's entire strategy is a conviction about the trajectory of open-source AI models — namely, that they're getting good enough, fast enough, to unlock massive enterprise adoption through fine-tuning.\n\"Both closed and open-source models are getting better and better in terms of quality,\" Haghighat said. \"We don't even need open source to surpass closed models, because as both of them are getting better, they unlock all these invisible lines of usefulness for different use cases.\"\nHe pointed to the proliferation of reinforcement learning and supervised fine-tuning techniques that allow companies to take an open-source model and make it \"as good as the closed model, not at everything, but at this narrow band of capability that they want.\"\nThat trend is already visible in Baseten's Model APIs business, launched alongside Training earlier this year to provide production-grade access to open-source models. The company was the first provider to offer access to DeepSeek V3 and R1, and has since added models like Llama 4 and Qwen 3, optimized for performance and reliability. Model APIs serves as a top-of-funnel product: companies start with off-the-shelf open-source models, realize they need customization, move to Training for fine-tuning, and ultimately deploy on Baseten's Dedicated Deployments infrastructure.\nYet Haghighat acknowledged the market remains \"fuzzy\" around which training techniques will dominate. Baseten is hedging by staying close to the bleeding edge through its Forward Deployed Engineering team, which works hands-on with select customers on reinforcement learning, supervised fine-tuning, and other advanced techniques.\n\"As we do that, we will see patterns emerge about what a productized training product can look like that really addresses the user's needs without them having to learn too much about how RL works,\" he said. \"Are we there as an industry? I would say not quite. I see some attempts at that, but they all seem like almost falling to the same trap that Blueprints fell into—a bit of a walled garden that ties the hands of AI folks behind their back.\"\nThe roadmap ahead includes potential abstractions for common training patterns, expansion into image, audio, and video fine-tuning, and deeper integration of advanced techniques like prefill-decode disaggregation, which separates the initial processing of prompts from token generation to improve efficiency.\nBaseten faces crowded field but bets developer experience and performance will win enterprise customers\nBaseten enters an increasingly crowded market for AI infrastructure. Hyperscalers like AWS, Google Cloud, and Microsoft Azure offer GPU compute for training, while specialized providers like Lambda Labs, CoreWeave, and Together AI compete on price, performance, or ease of use. Then there are vertically integrated platforms like Hugging Face, Replicate, and Modal that bundle training, inference, and model hosting.\nBaseten's differentiation rests on three pillars: its MCM system for multi-cloud capacity management, deep performance optimization expertise built from its inference business, and a developer experience tailored for production deployments rather than experimentation.\nThe company's recent $150 million Series D and $2.15 billion valuation provide runway to invest in both products simultaneously. Major customers include Descript, which uses Baseten for transcription workloads; Decagon, which runs customer service AI; and Sourcegraph, which powers coding assistants. All three operate in domains where model customization and performance are competitive advantages.\nTiming may be Baseten's biggest asset. The confluence of improving open-source models, enterprise discomfort with dependence on proprietary AI providers, and growing sophistication around fine-tuning techniques creates what Haghighat sees as a sustainable market shift.\n\"There is a lot of use cases for which closed models have gotten there and open ones have not,\" he said. \"Where I'm seeing in the market is people using different training techniques — more recently, a lot of reinforcement learning and SFT — to be able to get this open model to be as good as the closed model, not at everything, but at this narrow band of capability that they want. That's very palpable in the market.\"\nFor enterprises navigating the complex transition from closed to open AI models, Baseten's positioning offers a clear value proposition: infrastructure that handles the messy middle of fine-tuning while optimizing for the ultimate goal of performant, reliable, cost-effective inference at scale. The company's insistence that customers own their model weights — a stark contrast to competitors using training as a lock-in mechanism — reflects confidence that technical excellence, not contractual restrictions, will drive retention.\nWhether Baseten can execute on this vision depends on navigating tensions inherent in its strategy: staying at the infrastructure layer without becoming consultants, providing power and flexibility without overwhelming users with complexity, and building abstractions at exactly the right level as the market matures. The company's willingness to kill Blueprints when it failed suggests a pragmatism that could prove decisive in a market where many infrastructure providers over-promise and under-deliver.\n\"Through and through, we're an inference company,\" Haghighat emphasized. \"The reason that we did training is at the service of inference.\"\nThat clarity of purpose — treating training as a means to an end rather than an end in itself—may be Baseten's most important strategic asset. As AI deployment matures from experimentation to production, the companies that solve the full stack stand to capture outsized value. But only if they avoid the trap of technology in search of a problem.\nAt least Baseten's customers no longer have to SSH into boxes on Friday and pray their training jobs complete by Monday. In the infrastructure business, sometimes the best innovation is simply making the painful parts disappear.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "Baseten, an AI infrastructure company valued at $2.15 billion, has launched Baseten Training, a platform aimed at helping companies fine-tune open-source AI models while maintaining control over their model weights. This move addresses growing enterprise demand for alternatives to closed-source providers like OpenAI. By simplifying the training process and allowing easy access to model weights, Baseten aims to empower businesses to reduce reliance on expensive API calls. This shift is crucial as open-source models continue to improve, making them more viable for enterprise use.",
  "why_it_matters": [
    "Companies seeking to customize AI models can now do so without the headaches of managing complex infrastructure, which could lead to faster adoption of AI solutions.",
    "This launch signals a broader trend towards open-source AI, as enterprises look for cost-effective ways to leverage advanced models without being locked into proprietary systems."
  ],
  "lenses": {
    "eli12": "Baseten is making it easier for businesses to train their own AI models without getting bogged down in complicated tech. Think of it like a chef who can pick the freshest ingredients without worrying about the kitchen setup. This matters because it gives everyday companies the chance to create tailored AI solutions that fit their specific needs without the usual hassles.",
    "pm": "For product managers and founders, Baseten Training offers a way to meet user needs for customized AI without the burden of infrastructure management. This could lead to reduced costs and improved efficiency in deploying AI solutions. A practical implication is that businesses can focus on building their products while Baseten handles the underlying tech complexities.",
    "engineer": "From a technical perspective, Baseten Training supports multi-node training across NVIDIA GPU clusters with features like automated checkpointing and sub-minute job scheduling. This platform allows companies to retain ownership of their model weights, contrasting with competitors that impose restrictions. Such capabilities could streamline the deployment of custom AI models, enhancing performance and reliability."
  },
  "hype_meter": 4,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-11-11T03:58:55.490Z",
  "updated_at": "2025-11-11T03:58:55.490Z",
  "processing_order": 1762833535493
}