{
  "content_hash": "597683318054eff6abe4351fc8f0dfb79f482ab9757f713f48daba97b6c59358",
  "share_id": "doun0v",
  "title": "Databricks' OfficeQA uncovers disconnect: AI agents ace abstract tests but stall at 45% on enterprise docs",
  "optimized_headline": "Databricks' OfficeQA reveals AI success in tests but struggles with enterprise documents",
  "url": "https://venturebeat.com/data-infrastructure/databricks-officeqa-uncovers-disconnect-ai-agents-ace-abstract-tests-but",
  "source": "VentureBeat",
  "published_at": "2025-12-09T16:00:00.000Z",
  "raw_excerpt": "There is no shortage of AI benchmarks in the market today, with popular options like Humanity's Last Exam (HLE), ARC-AGI-2 and GDPval, among numerous others.\nAI agents excel at solving abstract math problems and passing PhD-level exams that most benchmarks are based on, but Databricks has a question for the enterprise: Can they actually handle the document-heavy work most enterprises need them to ",
  "raw_body": "There is no shortage of AI benchmarks in the market today, with popular options like Humanity's Last Exam (HLE), ARC-AGI-2 and GDPval, among numerous others.\nAI agents excel at solving abstract math problems and passing PhD-level exams that most benchmarks are based on, but Databricks has a question for the enterprise: Can they actually handle the document-heavy work most enterprises need them to do?\nThe answer, according to new research from the data and AI platform company, is sobering. Even the best-performing AI agents achieve less than 45% accuracy on tasks that mirror real enterprise workloads, exposing a critical gap between academic benchmarks and business reality.\n\"If we focus our research efforts on getting better at [existing benchmarks], then we're probably not solving the right problems to make Databricks a better platform,\" Erich Elsen, principal research scientist at Databricks, explained to VentureBeat. \"So that's why we were looking around. How do we create a benchmark that, if we get better at it, we're actually getting better at solving the problems that our customers have?\"\nThe result is OfficeQA, a benchmark designed to test AI agents on grounded reasoning: Answering questions based on complex proprietary datasets containing unstructured documents and tabular data. Unlike existing benchmarks that focus on abstract capabilities, OfficeQA proxies for the economically valuable tasks enterprises actually perform.\nWhy academic benchmarks miss the enterprise mark\nThere are numerous shortcomings of popular AI benchmarks from an enterprise perspective, according to Elsen.Â \nHLE features questions requiring PhD-level expertise across diverse fields. ARC-AGI evaluates abstract reasoning through visual manipulation of colored grids. Both push the frontiers of AI capabilities, but don't reflect daily enterprise work. Even GDPval, which was specifically created to evaluate economically useful tasks, misses the target.\n\"We come from a pretty heavy science or engineering background, and sometimes we create evals that reflect that,\" Elsen said. \" So they're either extremely math-heavy, which is a great, useful task, but advancing the frontiers of human mathematics is not what customers are trying to do with Databricks.\"\nWhile AI is commonly used for customer support and coding apps, Databricks' customer base has a broader set of requirements. Elsen noted that answering questions about documents or corpora of documents is a common enterprise task. These require parsing complex tables with nested headers, retrieving information across dozens or hundreds of documents and performing calculations where a single-digit error can cascade into organizations making incorrect business decisions.\nBuilding a benchmark that mirrors enterprise document complexity\nTo create a meaningful test of grounded reasoning capabilities, Databricks needed a dataset that approximates the messy reality of proprietary enterprise document corpora, while remaining freely available for research. The team landed on U.S. Treasury Bulletins, published monthly for five decades beginning in 1939 and quarterly thereafter.\nThe Treasury Bulletins check every box for enterprise document complexity. Each bulletin runs 100 to 200 pages and consists of prose, complex tables, charts and figures describing Treasury operations: Where federal money came from, where it went and how it financed government operations. The corpus spans approximately 89,000 pages across eight decades. Until 1996, the bulletins were scans of physical documents; afterwards, they were digitally produced PDFs. USAFacts, an organization whose mission is \"to make government data easier to access and understand,\" partnered with Databricks to develop the benchmark, identifying Treasury Bulletins as ideal and ensuring questions reflected realistic use cases.\nThe 246 questions require agents to handle messy, real-world document challenges: Scanned images, hierarchical table structures, temporal data spanning multiple reports and the need for external knowledge like inflation adjustments. Questions range from simple value lookups to multi-step analysis requiring statistical calculations and cross-year comparisons.\nTo ensure the benchmark requires actual document-grounded retrieval, Databricks filtered out questions that LLMs could answer using parametric knowledge or web search alone. This removed simpler questions and some surprisingly complex ones where models leveraged historical financial records memorized during pre-training.\nEvery question has a validated ground truth answer (typically a number, sometimes dates or small lists), enabling automated evaluation without human judging. This design choice matters: It allows reinforcement learning (RL) approaches that require verifiable rewards, similar to how models train on coding problems.\nCurrent performance exposes fundamental gaps\nDatabricks tested Claude Opus 4.5 Agent (using Claude's SDK) and GPT-5.1 Agent (using OpenAI's File Search API). The results should give pause to any enterprise betting heavily on current agent capabilities.\nWhen provided with raw PDF documents:\n\n Claude Opus 4.5 Agent (with default thinking=high) achieved 37.4% accuracy. \n\n GPT-5.1 Agent (with reasoning_effort=high) achieved 43.5% accuracy. \n\nHowever, performance improved noticeably when provided with pre-parsed versions of pages using Databricks' ai_parse_document, indicating that the poor raw PDF performance stems from LLM APIs struggling with parsing rather than reasoning. Even with parsed documents, the experiments show room for improvement.\nWhen provided with documents parsed using Databricks' ai_parse_document:\n\nClaude Opus 4.5 Agent achieved 67.8% accuracy (a +30.4 percentage point improvement)\n\nGPT-5.1 Agent achieved a 52.8% accuracy (a +9.3 percentage point improvement)\n\nThree findings that matter for enterprise deployments\nThe testing  identified critical insights for practitioners:\nParsing remains the fundamental blocker: Complex tables with nested headers, merged cells and unusual formatting frequently produce misaligned values. Even when given exact oracle pages, agents struggled primarily due to parsing errors, although performance roughly doubled with pre-parsed documents.\nDocument versioning creates ambiguity: Financial and regulatory documents get revised and reissued, meaning multiple valid answers exist depending on the publication date. Agents often stop searching once they find a plausible answer, missing more authoritative sources.\nVisual reasoning is a gap: About 3% of questions require chart or graph interpretation, where current agents consistently fail. For enterprises where data visualizations communicate critical insights, this represents a meaningful capability limitation.\nHow enterprises can use OfficeQA\nThe benchmark's design enables specific improvement paths beyond simple scoring. \n\"Since you're able to look at the right answer, it's easy to tell if the error is coming from parsing,\" Elsen explained. \nThis automated evaluation enables rapid iteration on parsing pipelines. The verified ground truth answers also enable RL training similar to coding benchmarks, since there's no human judgment required.\nElsen said the benchmark provides \"a really strong feedback signal\" for developers working on search solutions. However, he cautioned against treating it as training data.\n\"At least in my imagination, the goal of releasing this is more as an eval and not as a source of raw training data,\" he said. \"If you tune too specifically into this environment, then it's not clear how generalizable your agent results would be.\"\nWhat this means for enterprise AI deployments\nFor enterprises currently deploying or planning document-heavy AI agent systems, OfficeQA provides a sobering reality check. Even the latest frontier models achieve only 43% accuracy on unprocessed PDFs and fall short of 70% accuracy even with optimal document parsing. Performance on the hardest questions plateaus at 40%, indicating substantial room for improvement.\nThree immediate implications:\nEvaluate your document complexity: If your documents resemble the complexity profile of Treasury Bulletins (scanned images, nested table structures, cross-document references), expect accuracy well below vendor marketing claims. Test on your actual documents before production deployment.\nPlan for the parsing bottleneck: The test results indicate that parsing remains a fundamental blocker. Budget time and resources for custom parsing solutions rather than assuming off-the-shelf OCR will suffice.\n\nPlan for hard question failure modes: Even with optimal parsing, agents plateau at 40% on complex multi-step questions. For mission-critical document workflows that require multi-document analysis, statistical calculations or visual reasoning, current agent capabilities may not be ready without significant human oversight.\nFor enterprises looking to lead in AI-powered document intelligence, this benchmark provides a concrete evaluation framework and identifies specific capability gaps that need solving.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "Databricks has introduced OfficeQA, a benchmark revealing that AI agents struggle with real enterprise documents, achieving less than 45% accuracy. This starkly contrasts their high performance on abstract tests. For instance, Claude Opus 4.5 scored only 37.4% on raw PDFs, highlighting a significant gap between academic capabilities and practical needs. Understanding this disconnect is crucial for businesses relying on AI for document-heavy tasks.",
  "why_it_matters": [
    "Enterprises relying on AI for document handling may face significant accuracy issues, risking incorrect business decisions due to low performance on complex tasks.",
    "This research indicates a broader shift towards creating benchmarks that reflect real-world enterprise challenges, potentially reshaping AI development priorities."
  ],
  "lenses": {
    "eli12": "Databricks' OfficeQA shows that while AI can ace tests, it struggles with real-world documents. Imagine a student who excels in math but can't solve everyday problems. This gap affects businesses using AI for document-heavy tasks, emphasizing the need for more relevant testing.",
    "pm": "For product managers, OfficeQA highlights a critical user need: AI must improve in handling complex documents. With current models achieving under 45% accuracy, there's an opportunity to enhance product features that address parsing and reasoning challenges, ultimately improving user trust and satisfaction.",
    "engineer": "From a technical perspective, Databricks' tests reveal that parsing is a major limitation for AI agents. Claude Opus 4.5 and GPT-5.1 showed only 37.4% and 43.5% accuracy on unprocessed PDFs, respectively. However, accuracy improved significantly with pre-parsed documents, indicating that enhancing parsing algorithms could lead to better overall performance."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-12-10T04:08:11.249Z",
  "updated_at": "2025-12-10T04:08:11.249Z",
  "processing_order": 1765339691251
}