{
  "content_hash": "17b2b93ec3f89ba0fdf5f3c902d58a3c2b0c165a67e08f97ffdcfccd738af491",
  "share_id": "fvp420",
  "title": "From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration",
  "optimized_headline": "Revolutionary Framework Assesses House-Tree-Person Drawings with AI Collaboration",
  "url": "https://arxiv.org/abs/2512.21360",
  "source": "ArXiv AI",
  "published_at": "2025-12-29T05:00:00.000Z",
  "raw_excerpt": "arXiv:2512.21360v1 Announce Type: new \nAbstract: Background: The House-Tree-Person (HTP) drawing test, introduced by John Buck in 1948, remains a widely used projective technique in clinical psychology. However, it has long faced challenges such as heterogeneous scoring standards, reliance on examiners subjective experience, and a lack of a unified quantitative coding system.\n  Results: Quantitati",
  "raw_body": "arXiv:2512.21360v1 Announce Type: new \nAbstract: Background: The House-Tree-Person (HTP) drawing test, introduced by John Buck in 1948, remains a widely used projective technique in clinical psychology. However, it has long faced challenges such as heterogeneous scoring standards, reliance on examiners subjective experience, and a lack of a unified quantitative coding system.\n  Results: Quantitative experiments showed that the mean semantic similarity between Multimodal Large Language Model (MLLM) interpretations and human expert interpretations was approximately 0.75 (standard deviation about 0.05). In structurally oriented expert data sets, this similarity rose to 0.85, indicating expert-level baseline comprehension. Qualitative analyses demonstrated that the multi-agent system, by integrating social-psychological perspectives and destigmatizing narratives, effectively corrected visual hallucinations and produced psychological reports with high ecological validity and internal coherence.\n  Conclusions: The findings confirm the potential of multimodal large models as standardized tools for projective assessment. The proposed multi-agent framework, by dividing roles, decouples feature recognition from psychological inference and offers a new paradigm for digital mental-health services.\n  Keywords: House-Tree-Person test; multimodal large language model; multi-agent collaboration; cosine similarity; computational psychology; artificial intelligence",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "A new framework using multimodal large language models (MLLMs) aims to enhance the House-Tree-Person (HTP) drawing test in clinical psychology. The study found that MLLM interpretations had a mean semantic similarity of 0.75 with human experts, rising to 0.85 for expert data sets. This indicates that AI can effectively analyze psychological drawings, addressing issues of inconsistent scoring and subjective bias. This advancement could transform mental-health assessments by providing more standardized and reliable evaluations.",
  "why_it_matters": [
    "Clinicians could benefit from more consistent and objective assessments, improving the accuracy of psychological evaluations.",
    "This shift towards AI in mental health reflects a broader trend of integrating technology into traditional psychological practices, enhancing efficiency and reliability."
  ],
  "lenses": {
    "eli12": "The House-Tree-Person test helps psychologists understand a person's feelings through their drawings. This new AI framework works like a smart assistant, analyzing these drawings and providing consistent feedback. It matters because it could make mental health assessments more accurate and fair for everyone, reducing reliance on individual interpretations.",
    "pm": "For product managers and founders, this AI framework represents a user-friendly tool that meets the need for objective mental health assessments. By improving consistency and reducing interpretation bias, it could lower costs and enhance efficiency in clinical settings. This could lead to better user satisfaction and trust in psychological evaluations.",
    "engineer": "The study demonstrated that the MLLM achieved a mean semantic similarity of 0.75 with human experts, indicating a strong alignment in interpretation. In expert datasets, this similarity increased to 0.85, suggesting that the model can effectively understand complex psychological drawings. The multi-agent system's role in integrating diverse perspectives is crucial for improving the accuracy and validity of psychological assessments."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-12-30T04:12:35.393Z",
  "updated_at": "2025-12-30T04:12:35.393Z",
  "processing_order": 1767067955393
}