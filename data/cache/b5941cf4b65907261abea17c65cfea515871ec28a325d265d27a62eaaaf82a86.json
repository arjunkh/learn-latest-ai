{
  "content_hash": "b5941cf4b65907261abea17c65cfea515871ec28a325d265d27a62eaaaf82a86",
  "share_id": "tmcns6",
  "title": "Thinking Machines challenges OpenAI's AI scaling strategy: 'First superintelligence will be a superhuman learner'",
  "optimized_headline": "Thinking Machines Questions OpenAI's Strategy for Achieving Superhuman AI Learning",
  "url": "https://venturebeat.com/ai/thinking-machines-challenges-openais-ai-scaling-strategy-first",
  "source": "VentureBeat",
  "published_at": "2025-10-24T09:30:00.000Z",
  "raw_excerpt": "While the world's leading artificial intelligence companies race to build ever-larger models, betting billions that scale alone will unlock artificial general intelligence, a researcher at one of the industry's most secretive and valuable startups delivered a pointed challenge to that orthodoxy this week: The path forward isn't about training bigger — it's about learning better.\n\"I believe that th",
  "raw_body": "While the world's leading artificial intelligence companies race to build ever-larger models, betting billions that scale alone will unlock artificial general intelligence, a researcher at one of the industry's most secretive and valuable startups delivered a pointed challenge to that orthodoxy this week: The path forward isn't about training bigger — it's about learning better.\n\"I believe that the first superintelligence will be a superhuman learner,\" Rafael Rafailov, a reinforcement learning researcher at Thinking Machines Lab, told an audience at TED AI San Francisco on Tuesday. \"It will be able to very efficiently figure out and adapt, propose its own theories, propose experiments, use the environment to verify that, get information, and iterate that process.\"\nThis breaks sharply with the approach pursued by OpenAI, Anthropic, Google DeepMind, and other leading laboratories, which have bet billions on scaling up model size, data, and compute to achieve increasingly sophisticated reasoning capabilities. Rafailov argues these companies have the strategy backwards: what's missing from today's most advanced AI systems isn't more scale — it's the ability to actually learn from experience.\n\"Learning is something an intelligent being does,\" Rafailov said, citing a quote he described as recently compelling. \"Training is something that's being done to it.\"\nThe distinction cuts to the core of how AI systems improve — and whether the industry's current trajectory can deliver on its most ambitious promises. Rafailov's comments offer a rare window into the thinking at Thinking Machines Lab, the startup co-founded in February by former OpenAI chief technology officer Mira Murati that raised a record-breaking $2 billion in seed funding at a $12 billion valuation.\nWhy today's AI coding assistants forget everything they learned yesterday\nTo illustrate the problem with current AI systems, Rafailov offered a scenario familiar to anyone who has worked with today's most advanced coding assistants.\n\"If you use a coding agent, ask it to do something really difficult — to implement a feature, go read your code, try to understand your code, reason about your code, implement something, iterate — it might be successful,\" he explained. \"And then come back the next day and ask it to implement the next feature, and it will do the same thing.\"\nThe issue, he argued, is that these systems don't internalize what they learn. \"In a sense, for the models we have today, every day is their first day of the job,\" Rafailov said. \"But an intelligent being should be able to internalize information. It should be able to adapt. It should be able to modify its behavior so every day it becomes better, every day it knows more, every day it works faster — the way a human you hire gets better at the job.\"\nThe duct tape problem: How current training methods teach AI to take shortcuts instead of solving problems\nRafailov pointed to a specific behavior in coding agents that reveals the deeper problem: their tendency to wrap uncertain code in try/except blocks — a programming construct that catches errors and allows a program to continue running.\n\"If you use coding agents, you might have observed a very annoying tendency of them to use try/except pass,\" he said. \"And in general, that is basically just like duct tape to save the entire program from a single error.\"\nWhy do agents do this? \"They do this because they understand that part of the code might not be right,\" Rafailov explained. \"They understand there might be something wrong, that it might be risky. But under the limited constraint—they have a limited amount of time solving the problem, limited amount of interaction—they must only focus on their objective, which is implement this feature and solve this bug.\"\nThe result: \"They're kicking the can down the road.\"\nThis behavior stems from training systems that optimize for immediate task completion. \"The only thing that matters to our current generation is solving the task,\" he said. \"And anything that's general, anything that's not related to just that one objective, is a waste of computation.\"\nWhy throwing more compute at AI won't create superintelligence, according to Thinking Machines researcher\nRafailov's most direct challenge to the industry came in his assertion that continued scaling won't be sufficient to reach AGI.\n\"I don't believe we're hitting any sort of saturation points,\" he clarified. \"I think we're just at the beginning of the next paradigm—the scale of reinforcement learning, in which we move from teaching our models how to think, how to explore thinking space, into endowing them with the capability of general agents.\"\nIn other words, current approaches will produce increasingly capable systems that can interact with the world, browse the web, write code. \"I believe a year or two from now, we'll look at our coding agents today, research agents or browsing agents, the way we look at summarization models or translation models from several years ago,\" he said.\nBut general agency, he argued, is not the same as general intelligence. \"The much more interesting question is: Is that going to be AGI? And are we done — do we just need one more round of scaling, one more round of environments, one more round of RL, one more round of compute, and we're kind of done?\"\nHis answer was unequivocal: \"I don't believe this is the case. I believe that under our current paradigms, under any scale, we are not enough to deal with artificial general intelligence and artificial superintelligence. And I believe that under our current paradigms, our current models will lack one core capability, and that is learning.\"\nTeaching AI like students, not calculators: The textbook approach to machine learning\nTo explain the alternative approach, Rafailov turned to an analogy from mathematics education.\n\"Think about how we train our current generation of reasoning models,\" he said. \"We take a particular math problem, make it very hard, and try to solve it, rewarding the model for solving it. And that's it. Once that experience is done, the model submits a solution. Anything it discovers—any abstractions it learned, any theorems—we discard, and then we ask it to solve a new problem, and it has to come up with the same abstractions all over again.\"\nThat approach misunderstands how knowledge accumulates. \"This is not how science or mathematics works,\" he said. \"We build abstractions not necessarily because they solve our current problems, but because they're important. For example, we developed the field of topology to extend Euclidean geometry — not to solve a particular problem that Euclidean geometry couldn't handle, but because mathematicians and physicists understood these concepts were fundamentally important.\"\nThe solution: \"Instead of giving our models a single problem, we might give them a textbook. Imagine a very advanced graduate-level textbook, and we ask our models to work through the first chapter, then the first exercise, the second exercise, the third, the fourth, then move to the second chapter, and so on—the way a real student might teach themselves a topic.\"\nThe objective would fundamentally change: \"Instead of rewarding their success — how many problems they solved — we need to reward their progress, their ability to learn, and their ability to improve.\"\nThis approach, known as \"meta-learning\" or \"learning to learn,\" has precedents in earlier AI systems. \"Just like the ideas of scaling test-time compute and search and test-time exploration played out in the domain of games first\" — in systems like DeepMind's AlphaGo — \"the same is true for meta learning. We know that these ideas do work at a small scale, but we need to adapt them to the scale and the capability of foundation models.\"\nThe missing ingredients for AI that truly learns aren't new architectures—they're better data and smarter objectives\nWhen Rafailov addressed why current models lack this learning capability, he offered a surprisingly straightforward answer.\n\"Unfortunately, I think the answer is quite prosaic,\" he said. \"I think we just don't have the right data, and we don't have the right objectives. I fundamentally believe a lot of the core architectural engineering design is in place.\"\nRather than arguing for entirely new model architectures, Rafailov suggested the path forward lies in redesigning the data distributions and reward structures used to train models.\n\"Learning, in of itself, is an algorithm,\" he explained. \"It has inputs — the current state of the model. It has data and compute. You process it through some sort of structure, choose your favorite optimization algorithm, and you produce, hopefully, a stronger model.\"\nThe question: \"If reasoning models are able to learn general reasoning algorithms, general search algorithms, and agent models are able to learn general agency, can the next generation of AI learn a learning algorithm itself?\"\nHis answer: \"I strongly believe that the answer to this question is yes.\"\nThe technical approach would involve creating training environments where \"learning, adaptation, exploration, and self-improvement, as well as generalization, are necessary for success.\"\n\"I believe that under enough computational resources and with broad enough coverage, general purpose learning algorithms can emerge from large scale training,\" Rafailov said. \"The way we train our models to reason in general over just math and code, and potentially act in general domains, we might be able to teach them how to learn efficiently across many different applications.\"\nForget god-like reasoners: The first superintelligence will be a master student\nThis vision leads to a fundamentally different conception of what artificial superintelligence might look like.\n\"I believe that if this is possible, that's the final missing piece to achieve truly efficient general intelligence,\" Rafailov said. \"Now imagine such an intelligence with the core objective of exploring, learning, acquiring information, self-improving, equipped with general agency capability—the ability to understand and explore the external world, the ability to use computers, ability to do research, ability to manage and control robots.\"\nSuch a system would constitute artificial superintelligence. But not the kind often imagined in science fiction.\n\"I believe that intelligence is not going to be a single god model that's a god-level reasoner or a god-level mathematical problem solver,\" Rafailov said. \"I believe that the first superintelligence will be a superhuman learner, and it will be able to very efficiently figure out and adapt, propose its own theories, propose experiments, use the environment to verify that, get information, and iterate that process.\"\nThis vision stands in contrast to OpenAI's emphasis on building increasingly powerful reasoning systems, or Anthropic's focus on \"constitutional AI.\" Instead, Thinking Machines Lab appears to be betting that the path to superintelligence runs through systems that can continuously improve themselves through interaction with their environment.\nThe $12 billion bet on learning over scaling faces formidable challenges\nRafailov's appearance comes at a complex moment for Thinking Machines Lab. The company has assembled an impressive team of approximately 30 researchers from OpenAI, Google, Meta, and other leading labs. But it suffered a setback in early October when Andrew Tulloch, a co-founder and machine learning expert, departed to return to Meta after the company launched what The Wall Street Journal called a \"full-scale raid\" on the startup, approaching more than a dozen employees with compensation packages ranging from $200 million to $1.5 billion over multiple years.\nDespite these pressures, Rafailov's comments suggest the company remains committed to its differentiated technical approach. The company launched its first product, Tinker, an API for fine-tuning open-source language models, in October. But Rafailov's talk suggests Tinker is just the foundation for a much more ambitious research agenda focused on meta-learning and self-improving systems.\n\"This is not easy. This is going to be very difficult,\" Rafailov acknowledged. \"We'll need a lot of breakthroughs in memory and engineering and data and optimization, but I think it's fundamentally possible.\"\nHe concluded with a play on words: \"The world is not enough, but we need the right experiences, and we need the right type of rewards for learning.\"\nThe question for Thinking Machines Lab — and the broader AI industry — is whether this vision can be realized, and on what timeline. Rafailov notably did not offer specific predictions about when such systems might emerge.\nIn an industry where executives routinely make bold predictions about AGI arriving within years or even months, that restraint is notable. It suggests either unusual scientific humility — or an acknowledgment that Thinking Machines Lab is pursuing a much longer, harder path than its competitors.\nFor now, the most revealing detail may be what Rafailov didn't say during his TED AI presentation. No timeline for when superhuman learners might emerge. No prediction about when the technical breakthroughs would arrive. Just a conviction that the capability was \"fundamentally possible\" — and that without it, all the scaling in the world won't be enough.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "At a recent TED AI event, Rafael Rafailov from Thinking Machines Lab challenged the prevailing belief that larger AI models will lead to artificial general intelligence (AGI). He argued that the key to progress lies not in scaling but in enhancing AI's ability to learn from experience. This perspective matters now as the industry grapples with the limitations of current AI systems, which often fail to retain knowledge over time.",
  "why_it_matters": [
    "For developers and users of AI tools, this shift could lead to more adaptive and efficient coding assistants that improve over time, enhancing productivity.",
    "This challenge to the status quo signals a potential pivot in AI research, emphasizing learning mechanisms rather than sheer computational power, which could redefine industry priorities."
  ],
  "lenses": {
    "eli12": "Rafailov suggests that instead of just making AI bigger, we should focus on making it smarter at learning. Think of it like teaching a child: if they only memorize answers without understanding, they won't learn much. This matters because better learning could make AI tools more useful and effective in our daily lives.",
    "pm": "For product managers and founders, Rafailov's insights highlight a user need for AI that learns continuously rather than starting over each day. This could improve efficiency in product development and reduce costs. A practical implication is that investing in learning-focused AI could differentiate products in a crowded market.",
    "engineer": "From a technical perspective, Rafailov emphasizes that current AI models lack the ability to internalize knowledge, which limits their effectiveness. He advocates for a shift towards 'meta-learning,' where models learn how to learn, rather than just solving isolated problems. This approach could lead to more robust AI systems capable of generalizing knowledge across tasks."
  },
  "hype_meter": 4,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-10-25T03:50:55.250Z",
  "updated_at": "2025-10-25T03:50:55.250Z",
  "processing_order": 1761364255250
}