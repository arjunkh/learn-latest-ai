{
  "content_hash": "e882492cfdc90a00a60fdbfafc6876407b9ae14f6f32267c02d578393cfe20d2",
  "share_id": "mauva8",
  "title": "AI models are using material from retracted scientific papers",
  "optimized_headline": "AI Models Incorporate Content from Retired Scientific Papers: What It Means",
  "url": "https://www.technologyreview.com/2025/09/23/1123897/ai-models-are-using-material-from-retracted-scientific-papers/",
  "source": "MIT Technology Review",
  "published_at": "2025-09-23T09:00:00.000Z",
  "raw_excerpt": "Some AI chatbots rely on flawed research from retracted scientific papers to answer questions, according to recent studies. The findings, confirmed by MIT Technology Review, raise questions about how reliable AI tools are at evaluating scientific research and could complicate efforts by countries and industries seeking to invest in AI tools for scientists. AI search…",
  "raw_body": "Some AI chatbots rely on flawed research from retracted scientific papers to answer questions, according to recent studies. The findings, confirmed by MIT Technology Review, raise questions about how reliable AI tools are at evaluating scientific research and could complicate efforts by countries and industries seeking to invest in AI tools for scientists. AI search…",
  "category": "trends_risks_outlook",
  "category_confidence": "medium",
  "speedrun": "Recent studies reveal that some AI chatbots are using flawed information from retracted scientific papers to formulate responses. This raises serious concerns about the reliability of AI tools in assessing scientific research, a factor crucial for industries and governments investing in AI for scientific purposes. As these tools become more integrated into research workflows, ensuring their accuracy is increasingly important.",
  "why_it_matters": [
    "Researchers and institutions could face challenges when relying on AI, as it may provide inaccurate information based on discredited studies.",
    "This situation highlights a broader concern in the AI market about the quality of data used in training models, potentially affecting trust and investment."
  ],
  "lenses": {
    "eli12": "Some AI chatbots are using bad information from studies that have been taken back by scientists. Imagine using a recipe that has been proven to make you sick! This matters because if we can't trust AI to give us good scientific info, it could lead to poor decisions in health and technology.",
    "pm": "For product managers and founders, this issue indicates a need to ensure that AI tools are trained on reliable data. Users expect accurate information, and using flawed research could lead to costly mistakes. Focusing on data quality is crucial for building trust and user satisfaction.",
    "engineer": "From a technical standpoint, the reliance on retracted papers suggests that AI models may not be adequately filtering training data. This could lead to inaccuracies in outputs, as seen in studies confirming these issues. Developers might need to implement stricter data vetting processes to enhance the reliability of AI responses."
  },
  "hype_meter": 2,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-09-24T03:46:46.840Z",
  "updated_at": "2025-09-24T03:46:46.840Z",
  "processing_order": 1758685606842
}