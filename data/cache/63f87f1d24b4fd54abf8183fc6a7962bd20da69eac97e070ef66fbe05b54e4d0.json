{
  "content_hash": "63f87f1d24b4fd54abf8183fc6a7962bd20da69eac97e070ef66fbe05b54e4d0",
  "share_id": "ttt3kt",
  "title": "Tokenization takes the lead in the fight for data security",
  "optimized_headline": "\"How Tokenization is Revolutionizing Data Security: A Deep Dive\"",
  "url": "https://venturebeat.com/ai/tokenization-takes-the-lead-in-the-fight-for-data-security",
  "source": "VentureBeat",
  "published_at": "2025-12-15T15:00:00.000Z",
  "raw_excerpt": "Presented by Capital One Software\n\nTokenization is emerging as a cornerstone of modern data security, helping businesses separate the value of their data from its risk. During this VB in Conversation, Ravi Raghu, president, Capital One Software, talks about the ways tokenization can help reduce the value of breached data and preserve underlying data format and usability, including Capital One’s ow",
  "raw_body": "Presented by Capital One Software\n\nTokenization is emerging as a cornerstone of modern data security, helping businesses separate the value of their data from its risk. During this VB in Conversation, Ravi Raghu, president, Capital One Software, talks about the ways tokenization can help reduce the value of breached data and preserve underlying data format and usability, including Capital One’s own experience leveraging tokenization at scale. \nTokenization, Raghu asserts, is a far superior technology. It converts sensitive data into a nonsensitive digital replacement, called a token, that maps back to the original, which is secured in a digital vault. The token placeholder preserves both the format and the utility of the sensitive data, and can be used across applications — including AI models. Because tokenization removes the need to manage encryption keys or dedicate compute to constant encrypting and decrypting, it offers one of the most scalable ways for companies to protect their most sensitive data, he added.\n\"The killer part, from a security standpoint, when you think about it relative to other methods, if a bad actor gets hold of the data, they get hold of tokens,\" he explained. \"The actual data is not sitting with the token, unlike other methods like encryption, where the actual data sits there, just waiting for someone to get hold of a key or use brute force to get to the real data. From every angle this is the ideal way one ought to go about protecting sensitive data.\"\nThe tokenization differentiator \nMost organizations are just scratching the surface of data security, adding security at the very end, when data is read, to prevent an end user from accessing it. At minimum, organizations should focus on securing data on write, as it’s being stored. But best-in-class organizations go even further, protecting data at birth, the moment it’s created.\nAt one end of the safety spectrum is a simple lock-and-key approach that restricts access but leaves the underlying data intact. More advanced methods, like masking or modifying data, permanently alter its meaning — which can compromise its usefulness. File-level encryption provides broader protection for large volumes of stored data, but when you get down to field-level encryption (for example, a Social Security number), it becomes a bigger challenge. It takes a great deal of compute to encrypt a single field, and then to decrypt it at the point of usage. And still it has a fatal flaw: the original data is still right there, only needing the key to get access. \nTokenization avoids these pitfalls by replacing the original data with a surrogate that has no intrinsic value. If the token is intercepted — whether by the wrong person or the wrong machine — the data itself remains secure.\nThe business value of tokenization\n\"Fundamentally you’re protecting data, and that’s priceless,\" Raghu said. \"Another thing that’s priceless – can you use that for modeling purposes subsequently? On the one hand, it’s a protection thing, and on the other hand it’s a business enabling thing.\" \nBecause tokenization preserves the structure and ordinality of the original data, it can still be used for modeling and analytics, turning protection into a business enabler. Take private health data governed by HIPAA for example: tokenization means that data canbeused to build pricing models or for gene therapy research, while remaining compliant. \n\"If your data is already protected, you can then proliferate the usage of data across the entire enterprise and have everybody creating more and more value out of the data,\" Raghu said. \"Conversely, if you don’t have that, there’s a lot of reticence for enterprises today to have more people access it, or have more and more AI agents access their data. Ironically, they’re limiting the blast radius of innovation. The tokenization impact is massive, and there are many metrics you could use to measure that – operational impact, revenue impact, and obviously the peace of mind from a security standpoint.\"\nBreaking down adoption barriers\nUntil now, the fundamental challenge with traditional tokenization has been performance. AI requires a scale and speed that is unprecedented. That's one of the major challenges Capital One addresses with Databolt, its vaultless tokenization solution, which can produce up to 4 million tokens per second.\n\"Capital One has gone through tokenization for more than a decade. We started doing it because we’re serving our 100 million banking customers. We want to protect that sensitive data,\" Raghu said. \"We’ve eaten our own dog food with our internal tokenization capability, over 100 billion times a month. We’ve taken that know-how and that capability, scale, and speed, and innovated so that the world can leverage it, so that it’s a commercial offering.\"\nVaultless tokenization is an advanced form of tokenization that does not require a central database (vault) to store token mappings. Instead, it uses mathematical algorithms, cryptographic techniques, and deterministic mapping to generate tokens dynamically.This approach is faster, more scalable, and eliminates the security risk associated with managing a vault.\n\"We realized that for the scale and speed demands that we had, we needed to build out that capability ourselves,\" Raghu said. \"We’ve been iterating continuously on making sure that it can scale up to hundreds of billions of operations a month. All of our innovation has been around building IP and capability to do that thing at a battle-tested scale within our enterprise, for the purpose of serving our customers.\"\nWhile conventional tokenization methods can involve some complexity and slow down operations, Databolt seamlessly integrates with encrypted data warehouses, allowing businesses to maintain robust security without slowing performance or operations. Tokenization occurs in the customer’s environment, removing the need to communicate with an external network to perform tokenization operations, which can also slow performance.\n\"We believe that fundamentally, tokenization should be easy to adopt,\" Raghu said. \"You should be able to secure your data very quickly and operate at the speed and scale and cost needs that organizations have. I think that’s been a critical barrier so far for the mass scale adoption of tokenization. In an AI world, that’s going to become a huge enabler.\"\nDon't miss the whole conversation with Ravi Raghu, president, Capital One Software, here.\n\nSponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
  "category": "in_action_real_world",
  "category_confidence": "medium",
  "speedrun": "Tokenization is becoming essential for data security, allowing businesses to protect sensitive information by replacing it with non-sensitive tokens. This method not only secures data but also maintains its usability for applications like AI. Capital One’s vaultless tokenization solution, Databolt, can generate up to 4 million tokens per second, making it scalable and efficient. This shift is crucial as companies look to enhance security while enabling data-driven innovation.",
  "why_it_matters": [
    "Businesses can now protect sensitive data more effectively, reducing the risk of breaches while maintaining usability for analytics.",
    "The adoption of tokenization signals a broader shift towards more secure and efficient data management practices across industries."
  ],
  "lenses": {
    "eli12": "Tokenization works like a decoy in a magic trick. It replaces sensitive data with a harmless stand-in, keeping the real data safe. This is important for everyone because it means businesses can innovate and use data without risking privacy or security.",
    "pm": "For product managers and founders, tokenization addresses user needs for security while enhancing data utility. It could lower costs by eliminating the need for complex encryption processes, allowing for faster and more efficient operations, which is vital in a competitive market.",
    "engineer": "From a technical standpoint, Capital One's Databolt offers vaultless tokenization that generates tokens dynamically using algorithms, enabling up to 4 million tokens per second. This method reduces complexity and enhances performance, making it suitable for high-speed environments like AI applications."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-12-16T04:11:09.587Z",
  "updated_at": "2025-12-16T04:11:09.587Z",
  "processing_order": 1765858269588
}