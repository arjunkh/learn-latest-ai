{
  "content_hash": "805a987348e76c58f6e9d04df88122623acea6d0ae3ce19d65d0be6c21615d83",
  "share_id": "abtati",
  "title": "Antisocial behavior towards large language model users: experimental evidence",
  "optimized_headline": "Experimental Evidence Reveals Antisocial Behavior Toward Large Language Model Users",
  "url": "https://arxiv.org/abs/2601.09772",
  "source": "ArXiv AI",
  "published_at": "2026-01-16T05:00:00.000Z",
  "raw_excerpt": "arXiv:2601.09772v1 Announce Type: new \nAbstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I pro",
  "raw_body": "arXiv:2601.09772v1 Announce Type: new \nAbstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of \"no use\" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "A recent study explored how people react to users of large language models (LLMs). In an experiment with 491 participants, those who used LLMs faced a 36% reduction in their earnings from peers. This suggests that while LLMs can boost efficiency, they may also lead to social penalties for users. Understanding these dynamics is crucial as LLMs become more integrated into everyday tasks.",
  "why_it_matters": [
    "Users of LLMs could face social backlash, which might discourage their adoption in collaborative environments.",
    "This study highlights a broader trend where technological efficiency could lead to social stigma, affecting how AI tools are perceived in society."
  ],
  "lenses": {
    "eli12": "This study shows that people may not just dislike LLM users but also act on those feelings. Imagine if using a calculator made your classmates resent you for being 'cheaty.' This matters because it could shape how we use technology in schools and workplaces, impacting collaboration.",
    "pm": "For product managers, this finding indicates that user acceptance of LLMs could be influenced by social perceptions. If users fear backlash, they might avoid using your product. Addressing these concerns could enhance adoption and improve user experience.",
    "engineer": "The study employed a two-phase online experiment with 491 participants to analyze behaviors around LLM usage. It found that participants penalized LLM users more as their reliance on the model increased, indicating a credibility gap in self-reported usage. These insights could inform future model designs that consider social implications."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2026-01-17T04:07:20.108Z",
  "updated_at": "2026-01-17T04:07:20.108Z",
  "processing_order": 1768622840111
}