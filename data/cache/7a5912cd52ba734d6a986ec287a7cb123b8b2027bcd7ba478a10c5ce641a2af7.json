{
  "content_hash": "7a5912cd52ba734d6a986ec287a7cb123b8b2027bcd7ba478a10c5ce641a2af7",
  "share_id": "mnomei",
  "title": "MiniMax's new open M2.5 and M2.5 Lightning near state-of-the-art while costing 1/20th of Claude Opus 4.6",
  "optimized_headline": "MiniMax’s M2.5 models rival top tech at just 1/20th the price.",
  "url": "https://venturebeat.com/technology/minimaxs-new-open-m2-5-and-m2-5-lightning-near-state-of-the-art-while",
  "source": "VentureBeat",
  "published_at": "2026-02-12T20:28:00.000Z",
  "raw_excerpt": "Chinese AI startup MiniMax, headquartered in Shanghai, has sent shockwaves through the AI industry today with the release of its new M2.5 language model in two variants, which promise to make high-end artificial intelligence so cheap you might stop worrying about the bill entirely. \nIt was made open source on Hugging Face under a modified MIT License requiring that those using the model (or custom",
  "raw_body": "Chinese AI startup MiniMax, headquartered in Shanghai, has sent shockwaves through the AI industry today with the release of its new M2.5 language model in two variants, which promise to make high-end artificial intelligence so cheap you might stop worrying about the bill entirely. \nIt was made open source on Hugging Face under a modified MIT License requiring that those using the model (or custom variants) for commercial purposes \"prominently display 'MiniMax M2.5' on the user interface of such product or service.\" \nBut that's almost beside the point given how cheap MiniMax is serving it through its API and those of partners.\nFor the last few years, using the world’s most powerful AI was like hiring an expensive consultant—it was brilliant, but you watched the clock (and the token count) constantly. M2.5 changes that math, dropping the cost of the frontier by as much as 95%.\n\nBy delivering performance that rivals the top-tier models from Google and Anthropic at a fraction of the cost,  particularly in agentic tool use for enterprise tasks, including creating Microsoft Word, Excel and PowerPoint files, MiniMax is betting that the future isn't just about how smart a model is, but how often you can afford to use it.\nIndeed, to this end, MiniMax says it worked \"with senior professionals in fields such as finance, law, and social sciences\" to ensure the model could perform real work up to their specifications and standards.\nThis release matters because it signals a shift from AI as a \"chatbot\" to AI as a \"worker\". When intelligence becomes \"too cheap to meter,\" developers stop building simple Q&A tools and start building \"agents\"—software that can spend hours autonomously coding, researching, and organizing complex projects without breaking the bank.\nIn fact, MiniMax has already deployed this model into its own operations. Currently, 30% of all tasks at MiniMax HQ are completed by M2.5, and a staggering 80% of their newly committed code is generated by M2.5!\nAs the MiniMax team writes in their release blog post, \"we believe that M2.5 provides virtually limitless possibilities for the development and operation of agents in the economy.\"\nTechnology: sparse power and the CISPO breakthrough\nThe secret to M2.5’s efficiency lies in its Mixture of Experts (MoE) architecture. Rather than running all of its 230 billion parameters for every single word it generates, the model only \"activates\" 10 billion. This allows it to maintain the reasoning depth of a massive model while moving with the agility of a much smaller one.\nTo train this complex system, MiniMax developed a proprietary Reinforcement Learning (RL) framework called Forge. MiniMax engineer Olive Song stated on the ThursdAI podcast on YouTube that this technique was instrumental to scaling the performance even while using the relatively small number of parameters, and that the model was trained over a period of two months.\n\nForge is designed to help the model learn from \"real-world environments\" — essentially letting the AI practice coding and using tools in thousands of simulated workspaces. \n\"What we realized is that there's a lot of potential with a small model like this if we train reinforcement learning on it with a large amount of environments and agents,\" Song said. \"But it's not a very easy thing to do,\" adding that was what they spent \"a lot of time\" on.\nTo keep the model stable during this intense training, they used a mathematical approach called CISPO (Clipping Importance Sampling Policy Optimization) and shared the formula on their blog.\nThis formula ensures the model doesn't over-correct during training, allowing it to develop what MiniMax calls an \"Architect Mindset\". Instead of jumping straight into writing code, M2.5 has learned to proactively plan the structure, features, and interface of a project first.\nState-of-the-art (and near) benchmarks\nThe results of this architecture are reflected in the latest industry leaderboards. M2.5 hasn't just improved; it has vaulted into the top tier of coding models, approaching Anthropic's latest model, Claude Opus 4.6, released just a week ago, and showing that Chinese companies are now just days away from catching up to far better resourced (in terms of GPUs) U.S. labs.\nHere are some of the new MiniMax M2.5 benchmark highlights:\n\nSWE-Bench Verified: 80.2% — Matches Claude Opus 4.6 speeds\n\nBrowseComp: 76.3% — Industry-leading search & tool use.\n\nMulti-SWE-Bench: 51.3% — SOTA in multi-language coding\n\nBFCL (Tool Calling): 76.8% — High-precision agentic workflows.\n\nOn the ThursdAI podcast, host Alex Volkov pointed out that MiniMax M2.5 operates extremely quickly and therefore uses less tokens to complete tasks, on the order $0.15 per task compared to $3.00 for Claude Opus 4.6.\nBreaking the cost barrier\nMiniMax is offering two versions of the model through its API, both focused on high-volume production use:\n\nM2.5-Lightning: Optimized for speed, delivering 100 tokens per second. It costs $0.30 per 1M input tokens and $2.40 per 1M output tokens.\n\nStandard M2.5: Optimized for cost, running at 50 tokens per second. It costs half as much as the Lightning version ($0.15 per 1M input tokens / $1.20 per 1M output tokens).\n\nIn plain language: MiniMax claims you can run four \"agents\" (AI workers) continuously for an entire year for roughly $10,000. \nFor enterprise users, this pricing is roughly 1/10th to 1/20th the cost of competing proprietary models like GPT-5 or Claude 4.6 Opus.\n\n\nModel\n\nInput\n\nOutput\n\nTotal Cost\n\nSource\n\n\nQwen 3 Turbo\n\n$0.05\n\n$0.20\n\n$0.25\n\nAlibaba Cloud\n\n\ndeepseek-chat (V3.2-Exp)\n\n$0.28\n\n$0.42\n\n$0.70\n\nDeepSeek\n\n\ndeepseek-reasoner (V3.2-Exp)\n\n$0.28\n\n$0.42\n\n$0.70\n\nDeepSeek\n\n\nGrok 4.1 Fast (reasoning)\n\n$0.20\n\n$0.50\n\n$0.70\n\nxAI\n\n\nGrok 4.1 Fast (non-reasoning)\n\n$0.20\n\n$0.50\n\n$0.70\n\nxAI\n\n\nMiniMax M2.5\n\n$0.15\n\n$1.20\n\n$1.35\n\nMiniMax\n\n\nMiniMax M2.5-Lightning\n\n$0.30\n\n$2.40\n\n$2.70\n\nMiniMax\n\n\nGemini 3 Flash Preview\n\n$0.50\n\n$3.00\n\n$3.50\n\nGoogle\n\n\nKimi-k2.5\n\n$0.60\n\n$3.00\n\n$3.60\n\nMoonshot\n\n\nGLM-5\n\n$1.00\n\n$3.20\n\n$4.20\n\nZ.ai\n\n\nERNIE 5.0\n\n$0.85\n\n$3.40\n\n$4.25\n\nBaidu\n\n\nClaude Haiku 4.5\n\n$1.00\n\n$5.00\n\n$6.00\n\nAnthropic\n\n\nQwen3-Max (2026-01-23)\n\n$1.20\n\n$6.00\n\n$7.20\n\nAlibaba Cloud\n\n\nGemini 3 Pro (≤200K)\n\n$2.00\n\n$12.00\n\n$14.00\n\nGoogle\n\n\nGPT-5.2\n\n$1.75\n\n$14.00\n\n$15.75\n\nOpenAI\n\n\nClaude Sonnet 4.5\n\n$3.00\n\n$15.00\n\n$18.00\n\nAnthropic\n\n\nGemini 3 Pro (>200K)\n\n$4.00\n\n$18.00\n\n$22.00\n\nGoogle\n\n\nClaude Opus 4.6\n\n$5.00\n\n$25.00\n\n$30.00\n\nAnthropic\n\n\nGPT-5.2 Pro\n\n$21.00\n\n$168.00\n\n$189.00\n\nOpenAI\n\n\nStrategic implications for enterprises and leaders\nFor technical leaders, M2.5 represents more than just a cheaper API. It changes the operational playbook for enterprises right now.\nThe pressure to \"optimize\" prompts to save money is gone. You can now deploy high-context, high-reasoning models for routine tasks that were previously cost-prohibitive.\nThe 37% speed improvement in end-to-end task completion means the \"agentic\" pipelines valued by AI orchestrators — where models talk to other models — finally move fast enough for real-time user applications.\nIn addition, M2.5’s high scores in financial modeling (74.4% on MEWC) suggest it can handle the \"tacit knowledge\" of specialized industries like law and finance with minimal oversight.\nBecause M2.5 is positioned as an open-source model, organizations can potentially run intensive, automated code audits at a scale that was previously impossible without massive human intervention, all while maintaining better control over data privacy.\nMiniMax M2.5 is a signal that the frontier of AI is no longer just about who can build the biggest brain, but who can make that brain the most useful—and affordable—worker in the room.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "MiniMax, a Chinese AI startup, has launched its new M2.5 language model, available in two versions, which costs as little as 1/20th of competing models like Claude Opus 4.6. The M2.5 is open-source and designed to handle complex tasks in various fields, boasting a cost reduction of up to 95%. This shift from AI as a simple chatbot to a capable worker could change how businesses use AI, making it more accessible and practical for everyday tasks.",
  "why_it_matters": [
    "Businesses could benefit immediately from lower operational costs, allowing them to deploy AI for routine tasks without financial strain.",
    "This release indicates a broader trend in the market towards affordable, efficient AI solutions that can handle more complex, real-world applications."
  ],
  "lenses": {
    "eli12": "MiniMax's M2.5 model is like having a smart assistant that costs a fraction of what you'd pay for a consultant. It can perform tasks like coding and document creation at a much lower cost, making advanced AI accessible to more people. This matters because it allows everyday users and businesses to leverage powerful AI tools without worrying about high expenses.",
    "pm": "For product managers and founders, MiniMax's M2.5 could reshape user needs by providing a cost-effective AI solution for complex tasks. The significant cost savings and efficiency improvements allow for broader deployment of AI in everyday operations. This could lead to more innovative applications and services that were previously too expensive to implement.",
    "engineer": "Technically, MiniMax M2.5 uses a Mixture of Experts architecture, activating only 10 billion of its 230 billion parameters for efficiency. The model was trained using a proprietary Reinforcement Learning framework called Forge, which allows it to adapt to real-world tasks. This approach enables M2.5 to achieve high performance in coding benchmarks, rivaling top models while using fewer resources."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2026-02-14T04:57:00.298Z",
  "updated_at": "2026-02-14T04:57:00.298Z",
  "processing_order": 1771045020301
}