{
  "content_hash": "5fa7a3601bfeddfaca9ef268df4cf2e4e9c95ed34ca879be0001b9ef435ab793",
  "share_id": "odcnnu",
  "title": "OpenAI deploys Cerebras chips for 'near-instant' code generation in first major move beyond Nvidia",
  "optimized_headline": "OpenAI's Bold Shift: Cerebras Chips Enable Near-Instant Code Generation",
  "url": "https://venturebeat.com/technology/openai-deploys-cerebras-chips-for-15x-faster-code-generation-in-first-major",
  "source": "VentureBeat",
  "published_at": "2026-02-12T18:00:00.000Z",
  "raw_excerpt": "OpenAI on Thursday launched GPT-5.3-Codex-Spark, a stripped-down coding model engineered for near-instantaneous response times, marking the company's first significant inference partnership outside its traditional Nvidia-dominated infrastructure. The model runs on hardware from Cerebras Systems, a Sunnyvale-based chipmaker whose wafer-scale processors specialize in low-latency AI workloads.\nThe pa",
  "raw_body": "OpenAI on Thursday launched GPT-5.3-Codex-Spark, a stripped-down coding model engineered for near-instantaneous response times, marking the company's first significant inference partnership outside its traditional Nvidia-dominated infrastructure. The model runs on hardware from Cerebras Systems, a Sunnyvale-based chipmaker whose wafer-scale processors specialize in low-latency AI workloads.\nThe partnership arrives at a pivotal moment for OpenAI. The company finds itself navigating a frayed relationship with longtime chip supplier Nvidia, mounting criticism over its decision to introduce advertisements into ChatGPT, a newly announced Pentagon contract, and internal organizational upheaval that has seen a safety-focused team disbanded and at least one researcher resign in protest.\n\"GPUs remain foundational across our training and inference pipelines and deliver the most cost effective tokens for broad usage,\" an OpenAI spokesperson told VentureBeat. \"Cerebras complements that foundation by excelling at workflows that demand extremely low latency, tightening the end-to-end loop so use cases such as real-time coding in Codex feel more responsive as you iterate.\"\nThe careful framing — emphasizing that GPUs \"remain foundational\" while positioning Cerebras as a \"complement\" — underscores the delicate balance OpenAI must strike as it diversifies its chip suppliers without alienating Nvidia, the dominant force in AI accelerators.\nSpeed gains come with capability tradeoffs that OpenAI says developers will accept\nCodex-Spark represents OpenAI's first model purpose-built for real-time coding collaboration. The company claims the model delivers more than 1000 tokens per second when served on ultra-low latency hardware, though it declined to provide specific latency metrics such as time-to-first-token figures.\n\"We aren't able to share specific latency numbers, however Codex-Spark is optimized to feel near-instant — delivering more than 1000 tokens per second while remaining highly capable for real-world coding tasks,\" the OpenAI spokesperson said.\nThe speed gains come with acknowledged capability tradeoffs. On SWE-Bench Pro and Terminal-Bench 2.0 — two industry benchmarks that evaluate AI systems' ability to perform complex software engineering tasks autonomously — Codex-Spark underperforms the full GPT-5.3-Codex model. OpenAI positions this as an acceptable exchange: developers get responses fast enough to maintain creative flow, even if the underlying model cannot tackle the most sophisticated multi-step programming challenges.\nThe model launches with a 128,000-token context window and supports text only — no image or multimodal inputs. OpenAI has made it available as a research preview to ChatGPT Pro subscribers through the Codex app, command-line interface, and Visual Studio Code extension. A small group of enterprise partners will receive API access to evaluate integration possibilities.\n\"We are making Codex-Spark available in the API for a small set of design partners to understand how developers want to integrate Codex-Spark into their products,\" the spokesperson explained. \"We'll expand access over the coming weeks as we continue tuning our integration under real workloads.\"\nCerebras hardware eliminates bottlenecks that plague traditional GPU clusters\nThe technical architecture behind Codex-Spark tells a story about inference economics that increasingly matters as AI companies scale consumer-facing products. Cerebras's Wafer Scale Engine 3 — a single chip roughly the size of a dinner plate containing 4 trillion transistors — eliminates much of the communication overhead that occurs when AI workloads spread across clusters of smaller processors.\nFor training massive models, that distributed approach remains necessary and Nvidia's GPUs excel at it. But for inference — the process of generating responses to user queries — Cerebras argues its architecture can deliver results with dramatically lower latency. Sean Lie, Cerebras's CTO and co-founder, framed the partnership as an opportunity to reshape how developers interact with AI systems.\n\"What excites us most about GPT-5.3-Codex-Spark is partnering with OpenAI and the developer community to discover what fast inference makes possible — new interaction patterns, new use cases, and a fundamentally different model experience,\" Lie said in a statement. \"This preview is just the beginning.\"\nOpenAI's infrastructure team did not limit its optimization work to the Cerebras hardware. The company announced latency improvements across its entire inference stack that benefit all Codex models regardless of underlying hardware, including persistent WebSocket connections and optimizations within the Responses API. The results: 80 percent reduction in overhead per client-server round trip, 30 percent reduction in per-token overhead, and 50 percent reduction in time-to-first-token.\nA $100 billion Nvidia megadeal has quietly fallen apart behind the scenes\nThe Cerebras partnership takes on additional significance given the increasingly complicated relationship between OpenAI and Nvidia. Last fall, when OpenAI announced its Stargate infrastructure initiative, Nvidia publicly committed to investing $100 billion to support OpenAI as it built out AI infrastructure. The announcement appeared to cement a strategic alliance between the world's most valuable AI company and its dominant chip supplier.\nFive months later, that megadeal has effectively stalled, according to multiple reports. Nvidia CEO Jensen Huang has publicly denied tensions, telling reporters in late January that there is \"no drama\" and that Nvidia remains committed to participating in OpenAI's current funding round. But the relationship has cooled considerably, with friction stemming from multiple sources.\nOpenAI has aggressively pursued partnerships with alternative chip suppliers, including the Cerebras deal and separate agreements with AMD and Broadcom. From Nvidia's perspective, OpenAI may be using its influence to commoditize the very hardware that made its AI breakthroughs possible. From OpenAI's perspective, reducing dependence on a single supplier represents prudent business strategy.\n\"We will continue working with the ecosystem on evaluating the most price-performant chips across all use cases on an ongoing basis,\" OpenAI's spokesperson told VentureBeat. \"GPUs remain our priority for cost-sensitive and throughput-first use cases across research and inference.\" The statement reads as a careful effort to avoid antagonizing Nvidia while preserving flexibility — and reflects a broader reality that training frontier AI models still requires exactly the kind of massive parallel processing that Nvidia GPUs provide.\nDisbanded safety teams and researcher departures raise questions about OpenAI's priorities\nThe Codex-Spark launch comes as OpenAI navigates a series of internal challenges that have intensified scrutiny of the company's direction and values. Earlier this week, reports emerged that OpenAI disbanded its mission alignment team, a group established in September 2024 to promote the company's stated goal of ensuring artificial general intelligence benefits humanity. The team's seven members have been reassigned to other roles, with leader Joshua Achiam given a new title as OpenAI's \"chief futurist.\"\nOpenAI previously disbanded another safety-focused group, the superalignment team, in 2024. That team had concentrated on long-term existential risks from AI. The pattern of dissolving safety-oriented teams has drawn criticism from researchers who argue that OpenAI's commercial pressures are overwhelming its original non-profit mission.\nThe company also faces fallout from its decision to introduce advertisements into ChatGPT. Researcher Zoë Hitzig resigned this week over what she described as the \"slippery slope\" of ad-supported AI, warning in a New York Times essay that ChatGPT's archive of intimate user conversations creates unprecedented opportunities for manipulation. Anthropic seized on the controversy with a Super Bowl advertising campaign featuring the tagline: \"Ads are coming to AI. But not to Claude.\"\nSeparately, the company agreed to provide ChatGPT to the Pentagon through Genai.mil, a new Department of Defense program that requires OpenAI to permit \"all lawful uses\" without company-imposed restrictions — terms that Anthropic reportedly rejected. And reports emerged that Ryan Beiermeister, OpenAI's vice president of product policy who had expressed concerns about a planned explicit content feature, was terminated in January following a discrimination allegation she denies.\nOpenAI envisions AI coding assistants that juggle quick edits and complex autonomous tasks\nDespite the surrounding turbulence, OpenAI's technical roadmap for Codex suggests ambitious plans. The company envisions a coding assistant that seamlessly blends rapid-fire interactive editing with longer-running autonomous tasks — an AI that handles quick fixes while simultaneously orchestrating multiple agents working on more complex problems in the background.\n\"Over time, the modes will blend — Codex can keep you in a tight interactive loop while delegating longer-running work to sub-agents in the background, or fanning out tasks to many models in parallel when you want breadth and speed, so you don't have to choose a single mode up front,\" the OpenAI spokesperson told VentureBeat.\nThis vision would require not just faster inference but sophisticated task decomposition and coordination across models of varying sizes and capabilities. Codex-Spark establishes the low-latency foundation for the interactive portion of that experience; future releases will need to deliver the autonomous reasoning and multi-agent coordination that would make the full vision possible.\nFor now, Codex-Spark operates under separate rate limits from other OpenAI models, reflecting constrained Cerebras infrastructure capacity during the research preview. \"Because it runs on specialized low-latency hardware, usage is governed by a separate rate limit that may adjust based on demand during the research preview,\" the spokesperson noted. The limits are designed to be \"generous,\" with OpenAI monitoring usage patterns as it determines how to scale.\nThe real test is whether faster responses translate into better software\nThe Codex-Spark announcement arrives amid intense competition for AI-powered developer tools. Anthropic's Claude Cowork product triggered a selloff in traditional software stocks last week as investors considered whether AI assistants might displace conventional enterprise applications. Microsoft, Google, and Amazon continue investing heavily in AI coding capabilities integrated with their respective cloud platforms.\nOpenAI's Codex app has demonstrated rapid adoption since launching ten days ago, with more than one million downloads and weekly active users growing 60 percent week-over-week. More than 325,000 developers now actively use Codex across free and paid tiers. But the fundamental question facing OpenAI — and the broader AI industry — is whether speed improvements like those promised by Codex-Spark translate into meaningful productivity gains or merely create more pleasant experiences without changing outcomes.\nEarly evidence from AI coding tools suggests that faster responses encourage more iterative experimentation. Whether that experimentation produces better software remains contested among researchers and practitioners alike. What seems clear is that OpenAI views inference latency as a competitive frontier worth substantial investment, even as that investment takes it beyond its traditional Nvidia partnership into untested territory with alternative chip suppliers.\nThe Cerebras deal is a calculated bet that specialized hardware can unlock use cases that general-purpose GPUs cannot cost-effectively serve. For a company simultaneously battling competitors, managing strained supplier relationships, and weathering internal dissent over its commercial direction, it is also a reminder that in the AI race, standing still is not an option. OpenAI built its reputation by moving fast and breaking conventions. Now it must prove it can move even faster — without breaking itself.",
  "category": "in_action_real_world",
  "category_confidence": "medium",
  "speedrun": "OpenAI has launched GPT-5.3-Codex-Spark, a new coding model designed for near-instant responses, utilizing Cerebras chips instead of Nvidia's. This model can generate over 1000 tokens per second, enhancing real-time coding collaboration, although it sacrifices some capabilities compared to its predecessor. This shift is significant as OpenAI seeks to diversify its chip suppliers amidst a complicated relationship with Nvidia, aiming to maintain competitive edge in AI development.",
  "why_it_matters": [
    "Developers will benefit from faster coding responses, which could enhance productivity and creativity in software development.",
    "This move signals a broader trend of AI companies exploring alternative hardware solutions, potentially reshaping the competitive landscape in AI infrastructure."
  ],
  "lenses": {
    "eli12": "OpenAI's new coding model, Codex-Spark, is like upgrading from a bicycle to a sports car for coding tasks. It promises incredibly fast responses, making coding feel more fluid and responsive. This matters to everyday people because it could lead to quicker software updates and better apps, enhancing our digital experiences.",
    "pm": "For product managers, Codex-Spark offers an opportunity to meet user demands for speed in coding tools. The shift to Cerebras chips could reduce costs and improve efficiency in delivering AI features. This means teams might be able to iterate faster on product development, potentially leading to more innovative solutions.",
    "engineer": "From a technical perspective, Codex-Spark leverages Cerebras's Wafer Scale Engine 3, which optimizes inference by reducing communication overhead typical in GPU clusters. While it excels in low-latency tasks, it does underperform on complex benchmarks compared to the full GPT-5.3 model. This trade-off highlights the ongoing challenge of balancing speed and capability in AI development."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2026-02-13T05:09:56.504Z",
  "updated_at": "2026-02-13T05:09:56.504Z",
  "processing_order": 1770959396506
}