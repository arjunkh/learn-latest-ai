{
  "content_hash": "4a1acd74ecabcb632425fab3181f1dafcfbf6d424bef4683dcc77008fc1e78ab",
  "share_id": "etsu2r",
  "title": "Estimating the Self-Consistency of LLMs",
  "optimized_headline": "\"How Consistent Are Large Language Models? New Insights Revealed\"",
  "url": "https://arxiv.org/abs/2509.19489",
  "source": "ArXiv AI",
  "published_at": "2025-09-25T04:00:00.000Z",
  "raw_excerpt": "arXiv:2509.19489v1 Announce Type: new \nAbstract: Systems often repeat the same prompt to large language models (LLMs) and aggregate responses to improve reliability. This short note analyzes an estimator of the self-consistency of LLMs and the tradeoffs it induces under a fixed compute budget $B=mn$, where $m$ is the number of prompts sampled from the task distribution and $n$ is the number of rep",
  "raw_body": "arXiv:2509.19489v1 Announce Type: new \nAbstract: Systems often repeat the same prompt to large language models (LLMs) and aggregate responses to improve reliability. This short note analyzes an estimator of the self-consistency of LLMs and the tradeoffs it induces under a fixed compute budget $B=mn$, where $m$ is the number of prompts sampled from the task distribution and $n$ is the number of repeated LLM calls per prompt; the resulting analysis favors a rough split $m,n\\propto\\sqrt{B}$.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "Unable to summarize article at this time.",
  "why_it_matters": [
    "Summary unavailable",
    "Please check original source"
  ],
  "lenses": {
    "eli12": "We couldn't process this article right now.",
    "pm": "Article processing failed - check the original source for details.",
    "engineer": "JSON parsing error - the AI response was malformed."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-09-26T03:47:25.278Z",
  "updated_at": "2025-09-26T03:47:25.278Z",
  "processing_order": 1758858445280
}