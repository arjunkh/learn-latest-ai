{
  "content_hash": "00d89d438b50f52faeb2f385dc55b02dd2e9e4010feaddd69af2c9de38ff3bc7",
  "share_id": "mbs4mu",
  "title": "AI models block 87% of single attacks, but just 8% when attackers persist",
  "optimized_headline": "AI Models Stop 87% of Attacks, But Fail Against Persistent Threats",
  "url": "https://venturebeat.com/security/ai-models-block-87-of-single-attacks-but-just-8-when-attackers-persist",
  "source": "VentureBeat",
  "published_at": "2025-12-01T17:00:00.000Z",
  "raw_excerpt": "One malicious prompt gets blocked, while ten prompts get through. That gap defines the difference between passing benchmarks and withstanding real-world attacks — and it's a gap most enterprises don't know exists.\nWhen attackers send a single malicious request, open-weight AI models hold the line well, blocking attacks 87% of the time (on average). But when those same attackers send multiple promp",
  "raw_body": "One malicious prompt gets blocked, while ten prompts get through. That gap defines the difference between passing benchmarks and withstanding real-world attacks — and it's a gap most enterprises don't know exists.\nWhen attackers send a single malicious request, open-weight AI models hold the line well, blocking attacks 87% of the time (on average). But when those same attackers send multiple prompts across a conversation via probing, reframing and escalating across numerous exchanges, the math inverts fast. Attack success rates climb from 13% to 92%.\nFor CISOs evaluating open-weight models for enterprise deployment, the implications are immediate: The models powering your customer-facing chatbots, internal copilots and autonomous agents may pass single-turn safety benchmarks while failing catastrophically under sustained adversarial pressure.\n\"A lot of these models have started getting a little bit better,\" DJ Sampath, SVP of Cisco's AI software platform group, told VentureBeat. \"When you attack it once, with single-turn attacks, they're able to protect it. But when you go from single-turn to multi-turn, all of a sudden these models are starting to display vulnerabilities where the attacks are succeeding, almost 80% in some cases.\"\nWhy conversations break open-weight models open\nThe Cisco AI Threat Research and Security team found that open-weight AI models that block single attacks collapse under the weight of conversational persistence. Their recently published study shows that jailbreak success rates climb nearly tenfold when attackers extend the conversation. \nThe findings, published in \"Death by a Thousand Prompts: Open Model Vulnerability Analysis\" by Amy Chang, Nicholas Conley, Harish Santhanalakshmi Ganesan and Adam Swanda, quantify what many security researchers have long observed and suspected, but couldn't prove at scale.\nBut Cisco's research does, showing that treating multi-turn AI attacks as an extension of single-turn vulnerabilities misses the point entirely. The gap between them is categorical, not a matter of degree.\nThe research team evaluated eight open-weight models: Alibaba (Qwen3-32B), DeepSeek (v3.1), Google (Gemma 3-1B-IT), Meta (Llama 3.3-70B-Instruct), Microsoft (Phi-4), Mistral (Large-2), OpenAI (GPT-OSS-20b) and Zhipu AI (GLM 4.5-Air). Using black-box methodology — or testing without knowledge of internal architecture, which is exactly how real-world attackers operate — the team measured what happens when persistence replaces single-shot attacks.\nThe researchers note: \"Single-turn attack success rates (ASR) average 13.11%, as models can more readily detect and reject isolated adversarial inputs. In contrast, multi-turn attacks, leveraging conversational persistence, achieve an average ASR of 64.21% [a 5X increase], with some models like Alibaba Qwen3-32B reaching an 86.18% ASR and Mistral Large-2 reaching a 92.78% ASR.\" The latter was up 21.97% from a single-turn. \nThe results define the gap\nThe paper’s research team provides a succinct take on open-weight model resilience against attacks: \"This escalation, ranging from 2x to 10x, stems from models' inability to maintain contextual defenses over extended dialogues, allowing attackers to refine prompts and bypass safeguards.\"\nFigure 1: Single-turn attack success rates (blue) versus multi-turn success rates (red) across all eight tested models. The gap ranges from 10 percentage points (Google Gemma) to over 70 percentage points (Mistral, Llama, Qwen). Source: Cisco AI Defense \nThe five techniques that make persistence lethal\nThe research tested five multi-turn attack strategies, each exploiting a different aspect of conversational persistence. \n\nInformation decomposition and reassembly: Breaks harmful requests into innocuous components across turns, then reassemble them. Against Mistral Large-2, this technique achieved 95% success.\n\nContextual ambiguity introduces vague framing that confuses safety classifiers, reaching 94.78% success against Mistral Large-2.\n\nCrescendo attacks gradually escalate requests across turns, starting innocuously and building to harmful, hitting 92.69% success against Mistral Large-2.\n\nRole-play and persona adoption establish fictional contexts that normalize harmful outputs, achieving up to 92.44% success against Mistral Large-2.\n\nRefusal reframe repackages rejected requests with different justifications until one succeeds, reaching up to 89.15% success against Mistral Large-2.\n\nWhat makes these techniques effective isn't sophistication, it's familiarity. They mirror how humans naturally converse: building cBntext, clarifying requests and reframing when initial approaches fail. The models aren't vulnerable to exotic attacks. They're susceptible to persistence itself.\nTable 2: Attack success rates by technique across all models. The consistency across techniques means enterprises cannot defend against just one pattern. Source: Cisco AI Defense \nThe open-weight security paradox\nThis research lands at a critical inflection point as open source increasingly contributes to cybersecurity. Open-source and open-weight models have become foundational to the cybersecurity industry’s innovation. From accelerating startup time-to-market, reducing enterprise vendor lock-in and enabling customization that proprietary models can't match, open source is seen as the go-to platform by the majority of cybersecurity startups. \nThe paradox isn't lost on Cisco. The company's own Foundation-Sec-8B model, purpose-built for cybersecurity applications, is distributed as open weights on Hugging Face. Cisco isn't just criticizing competitors' models. The company is acknowledging a systemic vulnerability affecting the entire open-weight ecosystem, including models they themselves release. The message isn't \"avoid open-weight models.\" It's \"understand what you're deploying and add appropriate guardrails.\"\nSampath is direct about the implications: \"Open source has its own set of drawbacks. When you start to pull a model that is open weight, you have to think through what the security implications are and make sure that you're constantly putting the right types of guardrails around the model.\"\nTable 1: Attack success rates and security gaps across all tested models. Gaps exceeding 70% (Qwen at +73.48%, Mistral at +70.81%, Llama at +70.32%) represent high-priority candidates for additional guardrails before deployment. Source: Cisco AI Defense.\nWhy lab philosophy defines security outcomes\nThe security gap discovered by Cisco correlates directly with how AI labs approach alignment.\nTheir research makes this pattern clear: \"Models that focus on capabilities (e.g., Llama) did demonstrate the highest multi-turn gaps, with Meta explaining that developers are 'in the driver seat to tailor safety for their use case' in post-training. Models that focused heavily on alignment (e.g., Google Gemma-3-1B-IT) did demonstrate a more balanced profile between single- and multi-turn strategies deployed against it, indicating a focus on 'rigorous safety protocols' and 'low risk level' for misuse.\"\nCapability-first labs produce capability-first gaps. Meta's Llama shows a 70.32% security gap. Mistral's model card for Large-2 acknowledges it \"does not have any moderation mechanisms\" and shows a 70.81% gap. Alibaba's Qwen technical reports don't acknowledge safety or security concerns at all, and the model posts the highest gap at 73.48%.\nSafety-first labs produce smaller gaps. Google's Gemma emphasizes \"rigorous safety protocols\" and targets a \"low risk level\" for misuse. The outcome is the lowest gap at 10.53%, with more balanced performance across single- and multi-turn scenarios.\nModels optimized for capability and flexibility tend to arrive with less built-in safety. That's a design choice, and for many enterprise use cases, it's the right one. But enterprises need to recognize that \"capability-first\" often means \"security-second\" and budget accordingly.\nWhere attacks succeed most\nCisco tested 102 distinct subthreat categories. The top 15 achieved high success rates across all models, suggesting targeted defensive measures could deliver disproportionate security improvements.\nFigure 4: The 15 most vulnerable subthreat categories, ranked by average attack success rate. Malicious infrastructure operations leads at 38.8%, followed by gold trafficking (33.8%), network attack operations (32.5%) and investment fraud (31.2%). Source: Cisco AI Defense.\nFigure 2: Attack success rates across 20 threat categories and all eight models. Malicious code generation shows consistently high rates (3.1% to 43.1%), while model extraction attempts show near-zero success except for Microsoft Phi-4. Source: Cisco AI Defense.\nSecurity as the key to unlocking AI adoption\nSampath frames security not as an obstacle but as the mechanism that enables adoption: \"The way security folks inside enterprises are thinking about this is, 'I want to unlock productivity for all my users. Everybody's clamoring to use these tools. But I need the right guardrails in place because I don't want to show up in a Wall Street Journal piece,'\" he told VentureBeat. \nSampath continued, \"If we have the ability to see prompt injection attacks and block them, I can then unlock and unleash AI adoption in a fundamentally different fashion.\"\nWhat defense requires\nThe research points to six critical capabilities that enterprises should prioritize:\n\nContext-aware guardrails that maintain state across conversation turns\n\nModel-agnostic runtime protections\n\nContinuous red-teaming targeting multi-turn strategies\n\nHardened system prompts designed to resist instruction override\n\nComprehensive logging for forensic visibility\n\nThreat-specific mitigations for the top 15 subthreat categories identified in the research\n\nThe window for action\nSampath cautions against waiting: \"A lot of folks are in this holding pattern, waiting for AI to settle down. That is the wrong way to think about this. Every couple of weeks, something dramatic happens that resets that frame. Pick a partner and start doubling down.\"\nAs the report's authors conclude: \"The 2-10x superiority of multi-turn over single-turn attacks, model-specific weaknesses and high-risk threat patterns necessitate urgent action.\"\nTo repeat: One prompt gets blocked, 10 prompts get through. That equation won't change until enterprises stop testing single-turn defenses and start securing entire conversations.",
  "category": "in_action_real_world",
  "category_confidence": "medium",
  "speedrun": "Recent research by Cisco reveals a significant vulnerability in open-weight AI models: they block 87% of single attacks but only 8% of multi-turn attacks. This means that while models can fend off a single malicious prompt, they struggle when attackers persist through multiple exchanges, with success rates soaring to 92%. This gap highlights a critical need for enterprises to understand the limitations of their AI defenses, especially as they deploy these models in real-world applications.",
  "why_it_matters": [
    "CISOs must recognize that models appearing secure under single-turn tests can fail under sustained attacks, putting sensitive data at risk.",
    "This research signals a broader shift in AI security, emphasizing the need for comprehensive defenses against evolving adversarial techniques."
  ],
  "lenses": {
    "eli12": "Cisco's study shows that while AI models can block most single prompts, they struggle with ongoing conversations. Think of it like a guard who can stop one intruder but gets overwhelmed if ten people keep trying to sneak in. This is important because it affects how safe we feel using AI in everyday situations, like customer service or personal assistants.",
    "pm": "For product managers, this study underscores the importance of understanding user interactions with AI. While a model may seem effective at first glance, its performance can plummet under persistent attacks, impacting user trust. Ensuring robust defenses could enhance user experience and mitigate risks associated with deploying AI tools.",
    "engineer": "The research evaluates eight open-weight AI models, revealing stark differences in their resilience against attacks. For instance, the Mistral Large-2 model sees an attack success rate of 92.78% under multi-turn conditions, compared to just 13.11% for single attacks. This highlights the need for engineers to implement context-aware guardrails and continuous threat assessments to bolster security."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-12-02T04:05:50.407Z",
  "updated_at": "2025-12-02T04:05:50.407Z",
  "processing_order": 1764648350409
}