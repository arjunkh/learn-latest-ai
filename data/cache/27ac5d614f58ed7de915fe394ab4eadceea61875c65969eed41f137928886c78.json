{
  "content_hash": "27ac5d614f58ed7de915fe394ab4eadceea61875c65969eed41f137928886c78",
  "share_id": "mnoi3w",
  "title": "MiniMax's new open M2.5 and M2.5 Lightning near state-of-the-art while costing 1/20th of Claude Opus 4.6",
  "optimized_headline": "MiniMax’s M2.5 models rival Claude Opus 4.6 at 5% of the cost",
  "url": "https://venturebeat.com/technology/minimaxs-new-open-m2-5-and-m2-5-lightning-near-state-of-the-art-while",
  "source": "VentureBeat",
  "published_at": "2026-02-12T20:28:00.000Z",
  "raw_excerpt": "Chinese AI startup MiniMax, headquartered in Shanghai, has sent shockwaves through the AI industry today with the release of its new M2.5 language model in two variants, which promise to make high-end artificial intelligence so cheap you might stop worrying about the bill entirely. \nIt's also said to be \"open source,\" though the weights (settings) and code haven't been posted yet, nor has the exac",
  "raw_body": "Chinese AI startup MiniMax, headquartered in Shanghai, has sent shockwaves through the AI industry today with the release of its new M2.5 language model in two variants, which promise to make high-end artificial intelligence so cheap you might stop worrying about the bill entirely. \nIt's also said to be \"open source,\" though the weights (settings) and code haven't been posted yet, nor has the exact license type or terms. But that's almost beside the point given how cheap MiniMax is serving it through its API and those of partners.\nFor the last few years, using the world’s most powerful AI was like hiring an expensive consultant—it was brilliant, but you watched the clock (and the token count) constantly. M2.5 changes that math, dropping the cost of the frontier by as much as 95%.\n\nBy delivering performance that rivals the top-tier models from Google and Anthropic at a fraction of the cost,  particularly in agentic tool use for enterprise tasks, including creating Microsoft Word, Excel and PowerPoint files, MiniMax is betting that the future isn't just about how smart a model is, but how often you can afford to use it.\nIndeed, to this end, MiniMax says it worked \"with senior professionals in fields such as finance, law, and social sciences\" to ensure the model could perform real work up to their specifications and standards.\nThis release matters because it signals a shift from AI as a \"chatbot\" to AI as a \"worker\". When intelligence becomes \"too cheap to meter,\" developers stop building simple Q&A tools and start building \"agents\"—software that can spend hours autonomously coding, researching, and organizing complex projects without breaking the bank.\nIn fact, MiniMax has already deployed this model into its own operations. Currently, 30% of all tasks at MiniMax HQ are completed by M2.5, and a staggering 80% of their newly committed code is generated by M2.5!\nAs the MiniMax team writes in their release blog post, \"we believe that M2.5 provides virtually limitless possibilities for the development and operation of agents in the economy.\"\nTechnology: sparse power and the CISPO breakthrough\nThe secret to M2.5’s efficiency lies in its Mixture of Experts (MoE) architecture. Rather than running all of its 230 billion parameters for every single word it generates, the model only \"activates\" 10 billion. This allows it to maintain the reasoning depth of a massive model while moving with the agility of a much smaller one.\nTo train this complex system, MiniMax developed a proprietary Reinforcement Learning (RL) framework called Forge. MiniMax engineer Olive Song stated on the ThursdAI podcast on YouTube that this technique was instrumental to scaling the performance even while using the relatively small number of parameters, and that the model was trained over a period of two months.\n\nForge is designed to help the model learn from \"real-world environments\" — essentially letting the AI practice coding and using tools in thousands of simulated workspaces. \n\"What we realized is that there's a lot of potential with a small model like this if we train reinforcement learning on it with a large amount of environments and agents,\" Song said. \"But it's not a very easy thing to do,\" adding that was what they spent \"a lot of time\" on.\nTo keep the model stable during this intense training, they used a mathematical approach called CISPO (Clipping Importance Sampling Policy Optimization) and shared the formula on their blog.\nThis formula ensures the model doesn't over-correct during training, allowing it to develop what MiniMax calls an \"Architect Mindset\". Instead of jumping straight into writing code, M2.5 has learned to proactively plan the structure, features, and interface of a project first.\nState-of-the-art (and near) benchmarks\nThe results of this architecture are reflected in the latest industry leaderboards. M2.5 hasn't just improved; it has vaulted into the top tier of coding models, approaching Anthropic's latest model, Claude Opus 4.6, released just a week ago, and showing that Chinese companies are now just days away from catching up to far better resourced (in terms of GPUs) U.S. labs.\nHere are some of the new MiniMax M2.5 benchmark highlights:\n\nSWE-Bench Verified: 80.2% — Matches Claude Opus 4.6 speeds\n\nBrowseComp: 76.3% — Industry-leading search & tool use.\n\nMulti-SWE-Bench: 51.3% — SOTA in multi-language coding\n\nBFCL (Tool Calling): 76.8% — High-precision agentic workflows.\n\nOn the ThursdAI podcast, host Alex Volkov pointed out that MiniMax M2.5 operates extremely quickly and therefore uses less tokens to complete tasks, on the order $0.15 per task compared to $3.00 for Claude Opus 4.6.\nBreaking the cost barrier\nMiniMax is offering two versions of the model through its API, both focused on high-volume production use:\n\nM2.5-Lightning: Optimized for speed, delivering 100 tokens per second. It costs $0.30 per 1M input tokens and $2.40 per 1M output tokens.\n\nStandard M2.5: Optimized for cost, running at 50 tokens per second. It costs half as much as the Lightning version ($0.15 per 1M input tokens / $1.20 per 1M output tokens).\n\nIn plain language: MiniMax claims you can run four \"agents\" (AI workers) continuously for an entire year for roughly $10,000. \nFor enterprise users, this pricing is roughly 1/10th to 1/20th the cost of competing proprietary models like GPT-5 or Claude 4.6 Opus.\n\n\nModel\n\nInput\n\nOutput\n\nTotal Cost\n\nSource\n\n\nQwen 3 Turbo\n\n$0.05\n\n$0.20\n\n$0.25\n\nAlibaba Cloud\n\n\ndeepseek-chat (V3.2-Exp)\n\n$0.28\n\n$0.42\n\n$0.70\n\nDeepSeek\n\n\ndeepseek-reasoner (V3.2-Exp)\n\n$0.28\n\n$0.42\n\n$0.70\n\nDeepSeek\n\n\nGrok 4.1 Fast (reasoning)\n\n$0.20\n\n$0.50\n\n$0.70\n\nxAI\n\n\nGrok 4.1 Fast (non-reasoning)\n\n$0.20\n\n$0.50\n\n$0.70\n\nxAI\n\n\nMiniMax M2.5\n\n$0.15\n\n$1.20\n\n$1.35\n\nMiniMax\n\n\nMiniMax M2.5-Lightning\n\n$0.30\n\n$2.40\n\n$2.70\n\nMiniMax\n\n\nGemini 3 Flash Preview\n\n$0.50\n\n$3.00\n\n$3.50\n\nGoogle\n\n\nKimi-k2.5\n\n$0.60\n\n$3.00\n\n$3.60\n\nMoonshot\n\n\nGLM-5\n\n$1.00\n\n$3.20\n\n$4.20\n\nZ.ai\n\n\nERNIE 5.0\n\n$0.85\n\n$3.40\n\n$4.25\n\nBaidu\n\n\nClaude Haiku 4.5\n\n$1.00\n\n$5.00\n\n$6.00\n\nAnthropic\n\n\nQwen3-Max (2026-01-23)\n\n$1.20\n\n$6.00\n\n$7.20\n\nAlibaba Cloud\n\n\nGemini 3 Pro (≤200K)\n\n$2.00\n\n$12.00\n\n$14.00\n\nGoogle\n\n\nGPT-5.2\n\n$1.75\n\n$14.00\n\n$15.75\n\nOpenAI\n\n\nClaude Sonnet 4.5\n\n$3.00\n\n$15.00\n\n$18.00\n\nAnthropic\n\n\nGemini 3 Pro (>200K)\n\n$4.00\n\n$18.00\n\n$22.00\n\nGoogle\n\n\nClaude Opus 4.6\n\n$5.00\n\n$25.00\n\n$30.00\n\nAnthropic\n\n\nGPT-5.2 Pro\n\n$21.00\n\n$168.00\n\n$189.00\n\nOpenAI\n\n\nStrategic implications for enterprises and leaders\nFor technical leaders, M2.5 represents more than just a cheaper API. It changes the operational playbook for enterprises right now.\nThe pressure to \"optimize\" prompts to save money is gone. You can now deploy high-context, high-reasoning models for routine tasks that were previously cost-prohibitive.\nThe 37% speed improvement in end-to-end task completion means the \"agentic\" pipelines valued by AI orchestrators — where models talk to other models — finally move fast enough for real-time user applications.\nIn addition, M2.5’s high scores in financial modeling (74.4% on MEWC) suggest it can handle the \"tacit knowledge\" of specialized industries like law and finance with minimal oversight.\nBecause M2.5 is positioned as an open-source model, organizations can potentially run intensive, automated code audits at a scale that was previously impossible without massive human intervention, all while maintaining better control over data privacy, but until the licensing terms and weights are posted, this remains just a moniker. \nMiniMax M2.5 is a signal that the frontier of AI is no longer just about who can build the biggest brain, but who can make that brain the most useful—and affordable—worker in the room.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "MiniMax, a Chinese AI startup, has launched its new M2.5 language model, which is said to rival top-tier models like Claude Opus 4.6 while costing just 1/20th of the price. The model's unique architecture activates only a fraction of its parameters, allowing for high performance at a significantly lower cost—around $1.35 per task compared to $30 for competitors. This release marks a shift toward using AI as a worker rather than just a chatbot, potentially transforming enterprise operations.",
  "why_it_matters": [
    "Enterprises can now afford to deploy AI for complex tasks without worrying about costs, making advanced tools more accessible.",
    "This launch indicates a broader trend where companies focus on cost-effective AI solutions, potentially reshaping the competitive landscape."
  ],
  "lenses": {
    "eli12": "MiniMax's new M2.5 model is like having a smart assistant that can work tirelessly without racking up huge bills. With costs slashed to about $1.35 per task, it allows businesses to use AI for more than just simple questions. This change could make powerful AI tools available to everyone, not just big companies.",
    "pm": "For product managers, MiniMax's M2.5 offers a chance to rethink how AI can be integrated into products. With lower costs and high efficiency, it could be used for tasks previously deemed too expensive. This means teams could potentially enhance user experiences without breaking the bank.",
    "engineer": "Technically, M2.5 employs a Mixture of Experts architecture, activating only 10 billion out of its 230 billion parameters for each task. This design, combined with a proprietary Reinforcement Learning framework, allows it to achieve high performance while minimizing resource use. Its benchmark scores are competitive, with 80.2% on SWE-Bench, indicating it’s a serious contender in the AI landscape."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2026-02-13T05:09:47.973Z",
  "updated_at": "2026-02-13T05:09:47.973Z",
  "processing_order": 1770959387974
}