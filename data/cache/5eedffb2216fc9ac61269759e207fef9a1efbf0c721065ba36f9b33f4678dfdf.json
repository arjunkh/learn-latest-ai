{
  "content_hash": "5eedffb2216fc9ac61269759e207fef9a1efbf0c721065ba36f9b33f4678dfdf",
  "share_id": "ewcqte",
  "title": "Explaining AI Without Code: A User Study on Explainable AI",
  "optimized_headline": "Uncovering AI Insights: User Study Reveals Non-Coding Explanations of AI",
  "url": "https://arxiv.org/abs/2602.11159",
  "source": "ArXiv AI",
  "published_at": "2026-02-13T05:00:00.000Z",
  "raw_excerpt": "arXiv:2602.11159v1 Announce Type: new \nAbstract: The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate predictions, yet most methods demand technical expertise, limiting their value for novices. This gap i",
  "raw_body": "arXiv:2602.11159v1 Announce Type: new \nAbstract: The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate predictions, yet most methods demand technical expertise, limiting their value for novices. This gap is especially critical in no-code ML platforms, which seek to democratize AI but rarely include explainability. We present a human-centered XAI module in DashAI, an open-source no-code ML platform. The module integrates three complementary techniques, which are Partial Dependence Plots (PDP), Permutation Feature Importance (PFI), and KernelSHAP, into DashAI's workflow for tabular classification. A user study (N = 20; ML novices and experts) evaluated usability and the impact of explanations. Results show: (i) high task success ($\\geq80\\%$) across all explainability tasks; (ii) novices rated explanations as useful, accurate, and trustworthy on the Explanation Satisfaction Scale (ESS, Cronbach's $\\alpha$ = 0.74, a measure of internal consistency), while experts were more critical of sufficiency and completeness; and (iii) explanations improved perceived predictability and confidence on the Trust in Automation scale (TiA, $\\alpha$ = 0.60), with novices showing higher trust than experts. These findings highlight a central challenge for XAI in no-code ML, making explanations both accessible to novices and sufficiently detailed for experts.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "A new study highlights the need for explainable AI (XAI) in no-code machine learning platforms. Researchers integrated three techniques—Partial Dependence Plots, Permutation Feature Importance, and KernelSHAP—into DashAI to enhance understanding. In a user study with 20 participants, novices found the explanations useful and trustworthy, while experts sought more detail. This matters now as transparency in AI decisions is crucial, especially in sensitive fields like healthcare and finance.",
  "why_it_matters": [
    "Novices in machine learning can now use XAI tools to better understand automated decisions, improving their confidence in AI applications.",
    "The integration of explainable features in no-code platforms signals a shift towards making AI more accessible and trustworthy for a wider audience."
  ],
  "lenses": {
    "eli12": "This study shows how explainable AI can help everyday users understand automated decisions without needing technical skills. Think of it like a user-friendly guide that explains how a car engine works, making driving less intimidating. This matters because as AI becomes more common in our lives, understanding its decisions can help us trust it more.",
    "pm": "For product managers and founders, this study highlights the importance of integrating explainability into no-code platforms to meet user needs. By ensuring that both novices and experts can understand AI outputs, products could enhance user trust and satisfaction. A practical implication is the potential to attract a broader user base by reducing the intimidation factor of AI tools.",
    "engineer": "From a technical perspective, the study implemented three explainability techniques—PDP, PFI, and KernelSHAP—within DashAI, showing a high task success rate of 80% or more for explainability tasks. The findings suggest that while novices appreciated the explanations, experts demanded more depth, indicating a potential area for improvement in balancing simplicity and detail in XAI tools."
  },
  "hype_meter": 3,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2026-02-13T05:08:33.780Z",
  "updated_at": "2026-02-13T05:08:33.780Z",
  "processing_order": 1770959313780
}