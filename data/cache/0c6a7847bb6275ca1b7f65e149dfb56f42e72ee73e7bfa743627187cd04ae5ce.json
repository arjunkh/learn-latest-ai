{
  "content_hash": "0c6a7847bb6275ca1b7f65e149dfb56f42e72ee73e7bfa743627187cd04ae5ce",
  "share_id": "cffihs",
  "title": "Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents",
  "optimized_headline": "Revolutionary Co-EPG Framework Enhances Planning for Autonomous GUI Agents",
  "url": "https://arxiv.org/abs/2511.10705",
  "source": "ArXiv AI",
  "published_at": "2025-11-17T05:00:00.000Z",
  "raw_excerpt": "arXiv:2511.10705v1 Announce Type: new \nAbstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synth",
  "raw_body": "arXiv:2511.10705v1 Announce Type: new \nAbstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.",
  "category": "capabilities_and_how",
  "category_confidence": "medium",
  "speedrun": "A new framework called Co-EPG has been proposed to improve how autonomous GUI agents learn by integrating planning and grounding. This method creates a loop where the planning model learns from rewards provided by the grounding model, enhancing both over time. Notably, Co-EPG outperformed existing methods on benchmarks like Multimodal-Mind2Web after just three iterations without needing external data. This development could significantly advance GUI task automation in AI research.",
  "why_it_matters": [
    "This framework could immediately benefit developers of autonomous GUI agents, providing them with more efficient learning processes.",
    "At a market level, it signals a shift towards more integrated AI systems that learn from their own experiences rather than relying solely on external data."
  ],
  "lenses": {
    "eli12": "Co-EPG is like a team of players who learn from each other's moves to improve their game. In this case, the planning and grounding models help each other get better. This matters because it could lead to smarter, more adaptable AI that can handle tasks in ways that are closer to how humans think.",
    "pm": "For product managers, Co-EPG represents a way to enhance user experience in GUI automation. By improving efficiency and reducing reliance on external data, it could lower costs. This means products could become more responsive and user-friendly, meeting customer needs more effectively.",
    "engineer": "Technically, Co-EPG employs Group Relative Policy Optimization (GRPO) to create a feedback loop between planning and grounding models. It demonstrated superior performance on benchmarks like Multimodal-Mind2Web after only three iterations, showcasing its self-improvement capabilities. This approach highlights a shift towards self-sufficient training models that can optimize without external datasets."
  },
  "hype_meter": 2,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-11-18T03:55:47.600Z",
  "updated_at": "2025-11-18T03:55:47.600Z",
  "processing_order": 1763438147601
}