{
  "content_hash": "2b40a301cf95c54d90d33dbbe88cec8022bd63a59556a8c04f83c056e8eacc1d",
  "share_id": "gcttd6",
  "title": "Google Cloud takes aim at CoreWeave and AWS with managed Slurm for enterprise-scale AI training",
  "optimized_headline": "Google Cloud targets AWS and CoreWeave with new enterprise AI training tool",
  "url": "https://venturebeat.com/ai/google-cloud-takes-aim-at-coreweave-and-aws-with-managed-slurm-for",
  "source": "VentureBeat",
  "published_at": "2025-10-27T04:00:00.000Z",
  "raw_excerpt": "Some enterprises are best served by fine-tuning large models to their needs, but a number of companies plan to build their own models, a project that would require access to GPUs. \nGoogle Cloud wants to play a bigger role in enterprises’ model-making journey with its new service, Vertex AI Training. The service gives enterprises looking to train their own models access to a managed Slurm environme",
  "raw_body": "Some enterprises are best served by fine-tuning large models to their needs, but a number of companies plan to build their own models, a project that would require access to GPUs. \nGoogle Cloud wants to play a bigger role in enterprises’ model-making journey with its new service, Vertex AI Training. The service gives enterprises looking to train their own models access to a managed Slurm environment, data science tooling and any chips capable of large-scale model training. \nWith this new service, Google Cloud hopes to turn more enterprises away from other providers and encourage the building of more company-specific AI models. \nWhile Google Cloud has always offered the ability to customize its Gemini models, the new service allows customers to bring in their own models or customize any open-source model Google Cloud hosts. \nVertex AI Training positions Google Cloud directly against companies like CoreWeave and Lambda Labs, as well as its cloud competitors AWS and Microsoft Azure.  \nJaime de Guerre, senior director of product management at Gloogle Cloud, told VentureBeat that the company has been hearing from a lot of organizations of varying sizes that they need a way to better optimize compute but in a more reliable environment.\n“What we're seeing is that there's an increasing number of companies that are building or customizing large gen AI models to introduce a product offering built around those models, or to help power their business in some way,” de Guerre said. “This includes AI startups, technology companies, sovereign organizations building a model for a particular region or culture or language and some large enterprises that might be building it into internal processes.”\nDe Guerre noted that while anyone can technically use the service, Google is targeting companies planning large-scale model training rather than simple fine-tuning or LoRA adopters. Vertex AI Services will focus on longer-running training jobs spanning hundreds or even thousands of chips. Pricing will depend on the amount of compute the enterprise will need. \n“Vertex AI Training is not for adding more information to the context or using RAG; this is to train a model where you might start from completely random weights,” he said.\nModel customization on the rise\n\nEnterprises are recognizing the value of building customized models beyond just fine-tuning an LLM via retrieval-augmented generation (RAG). Custom models would know more in-depth company information and respond with answers specific to the organization. Companies like Arcee.ai have begun offering their models for customization to clients. Adobe recently announced a new service that allows enterprises to retrain Firefly for their specific needs. Organizations like FICO, which create small language models specific to the finance industry, often buy GPUs to train them at significant cost. \nGoogle Cloud said Vertex AI Training differentiates itself by giving access to a larger set of chips, services to monitor and manage training and the expertise it learned from training the Gemini models. \nSome early customers of Vertex AI Training include AI Singapore, a consortium of Singaporean research institutes and startups that built the 27-billion-parameter SEA-LION v4, and Salesforce’s AI research team. \nEnterprises often have to choose between taking an already-built LLM and fine-tuning it or building their own model. But creating an LLM from scratch is usually unattainable for smaller companies, or it simply doesn’t make sense for some use cases. However, for organizations where a fully custom or from-scratch model makes sense, the issue is gaining access to the GPUs needed to run training.\nModel training can be expensive\nTraining a model, de Guerre said, can be difficult and expensive, especially when organizations compete with several others for GPU space.\nHyperscalers like AWS and Microsoft — and, yes, Google — have pitched that their massive data centers and racks and racks of high-end chips deliver the most value to enterprises. Not only will they have access to expensive GPUs, but cloud providers often offer full-stack services to help enterprises move to production.\nServices like CoreWeave gained prominence for offering on-demand access to Nvidia H100s, giving customers flexibility in compute power when building models or applications. This has also given rise to a business model in which companies with GPUs rent out server space.\nDe Guerre said Vertex AI Training isn’t just about offering access to train models on bare compute, where the enterprise rents a GPU server; they also have to bring their own training software and manage the timing and failures. \n“This is a managed Slurm environment that will help with all the job scheduling and automatic recovery of jobs failing,” de Guerre said. “So if a training job slows down or stops due to a hardware failure, the training will automatically restart very quickly, based on automatic checkpointing that we do in management of the checkpoints to continue with very little downtime.”\nHe added that this provides higher throughput and more efficient training for a larger scale of compute clusters. \nServices like Vertex AI Training could make it easier for enterprises to build niche models or completely customize existing models. Still, just because the option exists doesn’t mean it's the right fit for every enterprise.",
  "category": "in_action_real_world",
  "category_confidence": "medium",
  "speedrun": "Google Cloud has launched Vertex AI Training, a managed Slurm environment designed for enterprises to train their own AI models. This service provides access to powerful GPUs and data science tools, positioning Google against competitors like AWS and CoreWeave. With this offering, Google aims to attract companies looking to create customized AI solutions, as many enterprises are shifting from merely fine-tuning existing models to building their own. This move is significant as it reflects the growing demand for tailored AI in various industries.",
  "why_it_matters": [
    "Enterprises can now access a more reliable and efficient environment for large-scale model training, which could enhance their AI capabilities significantly.",
    "This launch signals a broader shift in the market towards custom AI solutions, as companies increasingly seek to develop models tailored to their specific needs."
  ],
  "lenses": {
    "eli12": "Google Cloud's new Vertex AI Training service allows businesses to train their own AI models more easily. Think of it like a workshop where companies can build custom tools instead of just using off-the-shelf ones. This matters because it opens up opportunities for businesses to create AI that fits their unique needs, potentially improving their products and services.",
    "pm": "For product managers and founders, Vertex AI Training could streamline the process of developing custom AI models. It provides the necessary infrastructure and tools, potentially reducing costs and increasing efficiency in model training. This means companies can focus more on innovation rather than managing complex training environments.",
    "engineer": "From a technical standpoint, Vertex AI Training offers a managed Slurm environment, which automates job scheduling and recovery during model training. This service supports large-scale training jobs across hundreds or thousands of GPUs, enhancing throughput and efficiency. It's particularly advantageous for organizations that need to train models from scratch, minimizing downtime and improving resource utilization."
  },
  "hype_meter": 4,
  "model_meta": {
    "model": "gpt-4o-mini",
    "prompt_version": "v2.1"
  },
  "created_at": "2025-10-28T03:53:29.873Z",
  "updated_at": "2025-10-28T03:53:29.873Z",
  "processing_order": 1761623609875
}